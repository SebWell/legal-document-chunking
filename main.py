"""
FastAPI server for Legal Document Chunking
Designed for VPS deployment with n8n
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
import json
import re
import time
import hashlib
from datetime import datetime, timezone

app = FastAPI(
    title="Legal Document Chunking API",
    description="API pour le chunking intelligent de documents juridiques fran√ßais avec m√©tadonn√©es contextuelles",
    version="2.1.0"
)

# Force OpenAPI 3.0.2 pour compatibilit√© avec Swagger UI
def custom_openapi():
    if app.openapi_schema:
        return app.openapi_schema
    from fastapi.openapi.utils import get_openapi
    openapi_schema = get_openapi(
        title=app.title,
        version=app.version,
        description=app.description,
        routes=app.routes,
        openapi_version="3.0.2"
    )
    app.openapi_schema = openapi_schema
    return app.openapi_schema

app.openapi = custom_openapi

# CORS pour n8n
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChunkingRequest(BaseModel):
    extractedText: str
    userId: str
    projectId: str
    options: Optional[Dict[str, Any]] = {}

class DocumentMetadataExtractor:
    """Extracteur de m√©tadonn√©es documentaires pour le contexte RAG."""

    def __init__(self):
        self.document_patterns = {
            'contrat_reservation_vefa': {
                'title_patterns': [r'contrat.{0,20}r[e√©]servation.{0,20}vefa', r'r[e√©]servation.{0,30}futur.{0,10}ach[e√®]vement'],
                'parties_patterns': {
                    'reservant': [
                        r'soci√©t√©\s+d√©nomm√©e\s+([A-Z][^\n]+?)\s+au\s+capital',
                        r'r[e√©]servant[^\n]*([A-Z][^\n]{10,80})',
                        r'd√©nomm√©e\s+([A-Z][^\n]{20,80})'
                    ],
                    'reservataire': [
                        r'r[e√©]servataire[^\n]*([A-Z][^\n]{10,80})',
                        r'appel√©[e]?\s+r√©servataire[^\n]*([A-Z][^\n]{10,80})'
                    ]
                }
            },
            'cctp': {
                'title_patterns': [r'cctp', r'cahier.{0,20}clauses.{0,20}techniques', r'clauses.{0,20}techniques.{0,20}particuli[e√®]res'],
                'parties_patterns': {'maitre_ouvrage': r'ma[i√Æ]tre.{0,5}ouvrage[^\n]*([A-Z][^\n]{10,80})', 'entrepreneur': r'entrepreneur[^\n]*([A-Z][^\n]{10,80})'}
            },
            'bail_habitation': {
                'title_patterns': [r'bail.{0,20}habitation', r'bail.{0,20}location', r'contrat.{0,20}location'],
                'parties_patterns': {'bailleur': r'bailleur[^\n]*([A-Z][^\n]{10,80})', 'locataire': r'locataire[^\n]*([A-Z][^\n]{10,80})'}
            },
            'bail_commercial': {
                'title_patterns': [r'bail.{0,20}commercial', r'bail.{0,20}professionnel'],
                'parties_patterns': {'bailleur': r'bailleur[^\n]*([A-Z][^\n]{10,80})', 'preneur': r'preneur[^\n]*([A-Z][^\n]{10,80})'}
            },
            'acte_notarie': {
                'title_patterns': [r'acte.{0,20}notari[e√©]', r'acte.{0,20}vente', r'acte.{0,20}acquisition'],
                'parties_patterns': {'vendeur': r'vendeur[^\n]*([A-Z][^\n]{10,80})', 'acquereur': r'acqu[e√©]reur[^\n]*([A-Z][^\n]{10,80})'}
            },
            'permis_construire': {
                'title_patterns': [r'permis.{0,20}construire', r'autorisation.{0,20}construire'],
                'parties_patterns': {'demandeur': r'demandeur[^\n]*([A-Z][^\n]{10,80})', 'commune': r'commune.{0,20}([A-Z][^\n]{10,40})'}
            },
            'devis': {
                'title_patterns': [r'devis', r'estimation', r'chiffrage'],
                'parties_patterns': {'entreprise': r'entreprise[^\n]*([A-Z][^\n]{10,80})', 'client': r'client[^\n]*([A-Z][^\n]{10,80})'}
            }
        }

    def extract_document_metadata(self, text: str) -> Dict:
        """Extraction compl√®te des m√©tadonn√©es du document."""
        text_sample = text[:5000]  # Analyser les 5000 premiers caract√®res

        # 1. Classification du document
        doc_type = self.detect_document_type(text_sample)

        # 2. Extraction du titre
        title = self.extract_title(text_sample, doc_type)

        # 3. Extraction de la date principale
        date = self.extract_main_date(text_sample)

        # 4. Extraction des parties
        parties = self.extract_parties(text_sample, doc_type)

        # 5. Extraction de la localisation
        location = self.extract_location(text_sample)

        # 6. G√©n√©ration de l'ID standardis√©
        doc_id = self.generate_document_id(text, title, date)

        return {
            'id': doc_id,
            'title': title,
            'date': date,
            'type': doc_type,
            'parties': parties,
            'location': location,
            'project': self.extract_project_name(text_sample)
        }

    def detect_document_type(self, text: str) -> str:
        """D√©tecter le type de document avec patterns sp√©cialis√©s."""
        text_lower = text.lower()

        # Score par type de document
        scores = {}
        for doc_type, patterns in self.document_patterns.items():
            score = 0
            for pattern in patterns['title_patterns']:
                matches = len(re.findall(pattern, text_lower, re.IGNORECASE))
                score += matches * 3  # Bonus pour les patterns de titre

            # Bonus pour la pr√©sence des parties typiques
            for party_type, pattern in patterns['parties_patterns'].items():
                if re.search(pattern, text, re.IGNORECASE):
                    score += 2

            if score > 0:
                scores[doc_type] = score

        return max(scores, key=scores.get) if scores else 'contrat_general'

    def extract_title(self, text: str, doc_type: str) -> str:
        """Extraire le titre principal du document."""
        # Rechercher le titre en fonction du type d√©tect√©
        if doc_type in self.document_patterns:
            for pattern in self.document_patterns[doc_type]['title_patterns']:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    title = match.group(0).strip()
                    # Nettoyer et formater le titre
                    title = re.sub(r'\s+', ' ', title)
                    return title.upper()

        # Fallback: chercher les premiers mots en majuscules
        title_patterns = [
            r'^\s*([A-Z][A-Z\s]{20,100}?)(?:\n|$)',
            r'(?:^|\n)\s*([A-Z][A-Z\s]{15,80}?)(?:\n|$)',
            r'(?:CONTRAT|BAIL|CCTP|DEVIS|ACTE)\s+[A-Z\s]{10,60}'
        ]

        for pattern in title_patterns:
            match = re.search(pattern, text, re.MULTILINE)
            if match:
                title = match.group(1) if match.groups() else match.group(0)
                return re.sub(r'\s+', ' ', title.strip())

        return "DOCUMENT JURIDIQUE"

    def extract_main_date(self, text: str) -> str:
        """Extraire la date principale (signature, cr√©ation)."""
        # Patterns de dates avec contexte prioritaire
        priority_patterns = [
            r'sign[e√©]\s+le\s+(\d{1,2}[\s\/\-\.]+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre|\d{1,2})[\s\/\-\.]+\d{2,4})',
            r'fait\s+[√†a]\s+[^\n]+\s+le\s+(\d{1,2}[\s\/\-\.]+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre|\d{1,2})[\s\/\-\.]+\d{2,4})',
            r'\ben\s+date\s+du\s+(\d{1,2}[\s\/\-\.]+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre|\d{1,2})[\s\/\-\.]+\d{2,4})',
            r'√©tabli\s+le\s+(\d{1,2}[\s\/\-\.]+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre|\d{1,2})[\s\/\-\.]+\d{2,4})'
        ]

        for pattern in priority_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return self.normalize_date(match.group(1))

        # Fallback: premi√®re date trouv√©e
        general_patterns = [
            r'\d{1,2}[\s\/\-\.]+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)[\s\/\-\.]+\d{2,4}',
            r'\d{1,2}[\s\/\-\.]\d{1,2}[\s\/\-\.]\d{2,4}'
        ]

        for pattern in general_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return self.normalize_date(match.group(0))

        return datetime.now().strftime("%d/%m/%Y")

    def normalize_date(self, date_str: str) -> str:
        """Normaliser une date au format DD/MM/YYYY."""
        # Mapping des mois fran√ßais
        months = {
            'janvier': '01', 'f√©vrier': '02', 'mars': '03', 'avril': '04',
            'mai': '05', 'juin': '06', 'juillet': '07', 'ao√ªt': '08',
            'septembre': '09', 'octobre': '10', 'novembre': '11', 'd√©cembre': '12'
        }

        date_clean = re.sub(r'\s+', ' ', date_str.strip().lower())

        # Format avec mois en lettres
        for month_name, month_num in months.items():
            if month_name in date_clean:
                parts = re.split(r'[\s\-\.\/]+', date_clean)
                day = parts[0].zfill(2)
                year = parts[2] if len(parts[2]) == 4 else '20' + parts[2]
                return f"{day}/{month_num}/{year}"

        # Format num√©rique
        parts = re.split(r'[\s\-\.\/]+', date_clean)
        if len(parts) >= 3:
            day = parts[0].zfill(2)
            month = parts[1].zfill(2)
            year = parts[2] if len(parts[2]) == 4 else ('20' + parts[2] if int(parts[2]) < 50 else '19' + parts[2])
            return f"{day}/{month}/{year}"

        return date_clean

    def extract_parties(self, text: str, doc_type: str) -> Dict:
        """Extraire les parties du contrat selon le type de document."""
        parties = {}

        if doc_type in self.document_patterns:
            patterns = self.document_patterns[doc_type]['parties_patterns']
            for party_type, pattern_list in patterns.items():
                # Handle both list and single pattern formats
                patterns_to_try = pattern_list if isinstance(pattern_list, list) else [pattern_list]

                for pattern in patterns_to_try:
                    match = re.search(pattern, text, re.IGNORECASE)
                    if match:
                        party_name = match.group(1) if match.groups() else match.group(0)
                        # Nettoyer le nom de la partie
                        party_name = re.sub(r'[,:;].*', '', party_name).strip()
                        parties[party_type] = party_name[:80]  # Limiter la longueur
                        break  # Stop at first match for this party type

        # Add default reservataire for VEFA contracts if not found
        if doc_type == 'contrat_reservation_vefa' and 'reservataire' not in parties:
            parties['reservataire'] = '[R√©servataire]'

        # Fallback: recherche g√©n√©rique
        if not parties:
            generic_patterns = {
                'partie_1': r'd√©nomm√©[e]?\s+[¬´"]([^¬ª"\n]{10,80})[¬ª"]',
                'partie_2': r'd\'autre\s+part[^\n]*([A-Z][^\n]{10,80})'
            }

            for party_type, pattern in generic_patterns.items():
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    parties[party_type] = match.group(1).strip()[:80]

        return parties

    def extract_location(self, text: str) -> str:
        """Extraire la localisation principale."""
        location_patterns = [
            r'(?:situ√©|sis|localis√©)[^\n]*?([A-Z][a-z]+(?:[\s\-][A-Z][a-z]+)*(?:\s*\(\d{2,5}\))?)',
            r'commune\s+de\s+([A-Z][a-z]+(?:[\s\-][A-Z][a-z]+)*)',
            r'([A-Z][A-Z\s]+)\s*\(\d{2,5}\)',
            r'\b([A-Z][a-z]+(?:[\s\-][A-Z][a-z]+)*)\s*\(\d{2,5}\)'
        ]

        for pattern in location_patterns:
            match = re.search(pattern, text)
            if match:
                location = match.group(1).strip()
                # V√©rifier que ce n'est pas un nom de soci√©t√©
                if not re.match(r'(SARL|SAS|SA|SASU|EURL|SCI)', location, re.IGNORECASE):
                    return location

        return ""

    def extract_project_name(self, text: str) -> str:
        """Extraire le nom du projet/programme."""
        project_patterns = [
            r'programme[^\n]*[¬´"]([^¬ª"\n]{5,50})[¬ª"]',
            r'r√©sidence[^\n]*[¬´"]([^¬ª"\n]{5,50})[¬ª"]',
            r'projet[^\n]*[¬´"]([^¬ª"\n]{5,50})[¬ª"]',
            r'd√©nomm√©[e]?[^\n]*[¬´"]([^¬ª"\n]{5,50})[¬ª"]'
        ]

        for pattern in project_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()

        return ""

    def generate_document_id(self, text: str, title: str, date: str) -> str:
        """G√©n√©rer un ID standardis√© AAAAMMJJHHMMSSXXX."""
        # Utiliser la date du document si disponible, sinon date actuelle
        try:
            if date and '/' in date:
                day, month, year = date.split('/')[:3]
                doc_date = datetime(int(year), int(month), int(day), 12, 0, 0)
            else:
                doc_date = datetime.now()
        except:
            doc_date = datetime.now()

        # Format de base: AAAAMMJJHHMMSS
        base_id = doc_date.strftime("%Y%m%d%H%M%S")

        # G√©n√©rer un hash court du contenu pour l'unicit√©
        content_hash = hashlib.md5((text + title).encode('utf-8')).hexdigest()[:3].upper()

        return base_id + content_hash


class ChunkingService:
    """Service de chunking intelligent pour documents juridiques - Version Pro."""

    def __init__(self):
        # Dictionnaires sp√©cialis√©s pour l'immobilier
        self.real_estate_terms = {
            'construction': ['gros ≈ìuvre', 'second ≈ìuvre', 'fondations', 'charpente', 'couverture', '√©tanch√©it√©', 'isolation', 'cloisons', 'rev√™tements'],
            'legal_actors': ['ma√Ætre d\'ouvrage', 'ma√Ætre d\'≈ìuvre', 'architecte', 'entrepreneur', 'sous-traitant', 'bureau d\'√©tudes', 'contr√¥leur technique'],
            'insurance': ['dommages-ouvrage', 'd√©cennale', 'biennale', 'parfait ach√®vement', 'responsabilit√© civile', 'tous risques chantier'],
            'certifications': ['RT2012', 'RT2020', 'HQE', 'BBC', 'BEPOS', 'NF Habitat', 'Qualitel'],
            'procedures': ['permis de construire', 'd√©claration pr√©alable', 'DT-DICT', 'r√©ception des travaux', 'lev√©e des r√©serves'],
            'financial_terms': ['r√©vision de prix', 'actualisation', 'variation', 'acompte', 'avancement', 'retenue de garantie', 'd√©compte']
        }

        # Initialiser l'extracteur de m√©tadonn√©es
        self.metadata_extractor = DocumentMetadataExtractor()

    def create_smart_chunks(self, text: str, target_size: int = 60, overlap: int = 15, user_id: str = None, project_id: str = None):
        """Cr√©er des chunks intelligents avec gestion avanc√©e des structures."""
        # Nettoyage et pr√©paration du texte
        text = self.preprocess_text(text)

        # D√©tection des structures sp√©ciales
        special_sections = self.detect_special_structures(text)

        # Adaptation de la taille selon le contenu
        adaptive_size = self.calculate_adaptive_size(text, target_size)

        # Traitement selon la structure d√©tect√©e
        if special_sections['has_tables']:
            return self.chunk_with_table_handling(text, adaptive_size, overlap, user_id=user_id, project_id=project_id)
        else:
            return self.chunk_standard_content(text, adaptive_size, overlap, user_id=user_id, project_id=project_id)

    def preprocess_text(self, text: str) -> str:
        """Pr√©traitement avanc√© du texte."""
        # Nettoyage des espaces multiples
        text = re.sub(r'\s+', ' ', text.strip())

        # Normalisation des caract√®res sp√©ciaux
        text = re.sub(r'[\u00A0\u2000-\u200B\u2028\u2029]', ' ', text)

        # Protection des structures importantes
        text = self.protect_legal_structures(text)

        return text

    def protect_legal_structures(self, text: str) -> str:
        """Prot√©ger les structures juridiques importantes."""
        # Prot√©ger les r√©f√©rences d'articles
        text = re.sub(r'(article\s+[a-z]?\d+[\-\d]*)', r'\1üîí', text, flags=re.IGNORECASE)

        # Prot√©ger les montants
        text = re.sub(r'(\d+[\s,]*\d*\s*(?:euros?|‚Ç¨))', r'\1üîí', text, flags=re.IGNORECASE)

        return text

    def detect_special_structures(self, text: str) -> dict:
        """D√©tecter les structures sp√©ciales dans le document."""
        return {
            'has_tables': '|' in text or 'Nom / D√©nomination' in text,
            'has_numbered_clauses': bool(re.search(r'^\s*\d+[\./)]', text, re.MULTILINE)),
            'has_legal_formulas': bool(re.search(r'\$[^\$]+\$', text)),
            'has_complex_references': len(re.findall(r'article\s+[a-z]?\d+', text, re.IGNORECASE)) > 5
        }

    def calculate_adaptive_size(self, text: str, base_size: int) -> int:
        """Calculer la taille adaptative selon le type de contenu."""
        content_type = self.classify_global_content(text)

        size_adjustments = {
            'financial': base_size + 10,  # Plus de contexte pour les clauses financi√®res
            'technical_requirements': base_size + 15,  # Sp√©cifications techniques d√©taill√©es
            'obligations': base_size - 5,  # Obligations plus concises
            'timeline': base_size,
            'legal_references': base_size - 10  # Articles courts et pr√©cis
        }

        return size_adjustments.get(content_type, base_size)

    def classify_global_content(self, text: str) -> str:
        """Classification globale du document."""
        text_lower = text.lower()

        # Calcul des scores pour chaque cat√©gorie
        category_scores = {
            'financial': len(re.findall(r'prix|co√ªt|euros?|‚Ç¨|montant|facture|paiement|devis', text_lower)),
            'technical_requirements': len(re.findall(r'technique|norme|sp√©cification|mat√©riau|dtu|performance|qualit√©', text_lower)),
            'obligations': len(re.findall(r'obligation|engage|doit|responsabilit√©|devoir|tenu', text_lower)),
            'timeline': len(re.findall(r'd√©lai|livraison|√©ch√©ance|date|planning|dur√©e', text_lower)),
            'legal_references': len(re.findall(r'article|d√©cret|loi|code|r√©f√©rence', text_lower))
        }

        return max(category_scores, key=category_scores.get) if any(category_scores.values()) else 'general'

    def chunk_with_table_handling(self, text: str, target_size: int, overlap: int, user_id: str = None, project_id: str = None):
        """Chunking sp√©cialis√© pour les documents avec tableaux."""
        chunks = []
        chunk_id = 1

        # S√©paration du contenu par sections
        sections = self.split_by_tables(text)

        for section in sections:
            if self.is_table_content(section):
                # Traitement sp√©cialis√© pour les tableaux
                table_chunks = self.chunk_table_content(section, chunk_id, user_id, project_id)
                chunks.extend(table_chunks)
                chunk_id += len(table_chunks)
            else:
                # Traitement standard pour le texte normal
                section_chunks = self.chunk_standard_content(section, target_size, overlap, start_id=chunk_id, user_id=user_id, project_id=project_id)
                chunks.extend(section_chunks)
                chunk_id += len(section_chunks)

        return chunks

    def split_by_tables(self, text: str) -> list:
        """S√©parer le texte en sections normales et tableaux."""
        # Pattern pour d√©tecter les d√©buts de tableaux
        table_pattern = r'(\|[^|]*\|[^|]*\|)'

        sections = []
        last_end = 0

        for match in re.finditer(table_pattern, text):
            # Ajouter le texte avant le tableau
            if match.start() > last_end:
                sections.append(text[last_end:match.start()])

            # Trouver la fin du tableau
            table_start = match.start()
            table_end = self.find_table_end(text, table_start)
            sections.append(text[table_start:table_end])
            last_end = table_end

        # Ajouter le texte restant
        if last_end < len(text):
            sections.append(text[last_end:])

        return [s.strip() for s in sections if s.strip()]

    def find_table_end(self, text: str, start: int) -> int:
        """Trouver la fin d'un tableau."""
        lines = text[start:].split('\n')
        end_line = 0

        for i, line in enumerate(lines):
            if '|' not in line and line.strip() and not re.match(r'^\s*[-:]+\s*$', line):
                break
            end_line = i + 1

        return start + len('\n'.join(lines[:end_line]))

    def is_table_content(self, section: str) -> bool:
        """V√©rifier si une section contient un tableau."""
        return '|' in section and section.count('|') >= 4

    def chunk_table_content(self, table_text: str, start_id: int, user_id: str = None, project_id: str = None) -> list:
        """Chunking sp√©cialis√© pour les tableaux."""
        chunks = []
        lines = table_text.strip().split('\n')

        # Grouper les lignes de tableau en chunks logiques
        current_group = []
        header_processed = False

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # Conserver l'en-t√™te avec chaque groupe
            if not header_processed and '|' in line and not re.match(r'^\s*[-:]+\s*$', line):
                header = line
                header_processed = True
                current_group.append(line)
            elif re.match(r'^\s*[-:]+\s*$', line):
                current_group.append(line)
            else:
                current_group.append(line)

                # Cr√©er un chunk tous les 3-4 lignes de donn√©es
                if len(current_group) >= 4:
                    chunk_content = '\n'.join(current_group)
                    chunk = self.create_chunk(chunk_content, start_id)
                    chunks.append(chunk)

                    # Red√©marrer avec l'en-t√™te
                    current_group = [header] if 'header' in locals() else []
                    start_id += 1

        # Chunk final s'il reste des donn√©es
        if current_group and len(current_group) > 1:
            chunk_content = '\n'.join(current_group)
            chunk = self.create_chunk(chunk_content, start_id)
            chunks.append(chunk)

        return chunks

    def chunk_standard_content(self, text: str, target_size: int, overlap: int, start_id: int = 1, user_id: str = None, project_id: str = None):
        """Chunking standard am√©lior√©."""
        # D√©coupage par phrases avec gestion des abr√©viations
        sentences = self.smart_sentence_split(text)
        chunks = []
        current_chunk = []
        current_word_count = 0
        chunk_id = start_id

        for sentence in sentences:
            if not sentence.strip():
                continue

            sentence_words = sentence.split()

            # V√©rification de la coh√©sion avant de couper
            if (current_word_count + len(sentence_words) > target_size and
                current_chunk and self.can_split_here(sentence, current_chunk)):

                # Cr√©er le chunk actuel
                chunk_content = ' '.join(current_chunk)
                chunk = self.create_chunk(chunk_content, chunk_id, user_id, project_id)
                chunks.append(chunk)

                # Pr√©parer le chunk suivant avec overlap s√©mantique
                overlap_words = self.get_semantic_overlap(current_chunk, overlap)
                current_chunk = overlap_words + sentence_words
                current_word_count = len(current_chunk)
                chunk_id += 1
            else:
                # Ajouter la phrase au chunk actuel
                current_chunk.extend(sentence_words)
                current_word_count += len(sentence_words)

        # Chunk final
        if current_chunk:
            chunk_content = ' '.join(current_chunk)
            chunk = self.create_chunk(chunk_content, chunk_id, user_id, project_id)
            chunks.append(chunk)

        return chunks

    def smart_sentence_split(self, text: str) -> list:
        """D√©coupage intelligent des phrases avec gestion des abr√©viations."""
        # Prot√©ger les abr√©viations courantes
        abbreviations = ['art.', 'etc.', 'cf.', 'p.ex.', 'c.-√†-d.', 'M.', 'Mme', 'Dr.', 'Me']

        protected_text = text
        for abbr in abbreviations:
            protected_text = protected_text.replace(abbr, abbr.replace('.', 'üîí'))

        # D√©coupage par phrases
        sentences = re.split(r'(?<=[.!?])\s+', protected_text)

        # Restaurer les points
        sentences = [s.replace('üîí', '.') for s in sentences]

        return sentences

    def can_split_here(self, next_sentence: str, current_chunk: list) -> bool:
        """V√©rifier si on peut couper √† cet endroit sans nuire √† la coh√©sion."""
        if not current_chunk:
            return True

        # Ne pas couper si la phrase suivante commence par une conjonction
        conjunctions = ['et', 'ou', 'mais', 'donc', 'car', 'ainsi', 'alors', 'cependant', 'toutefois', 'n√©anmoins']
        first_word = next_sentence.strip().split()[0].lower() if next_sentence.strip() else ''

        if first_word in conjunctions:
            return False

        # Ne pas couper au milieu d'une √©num√©ration
        last_chunk_text = ' '.join(current_chunk[-10:])  # Derniers mots
        if re.search(r'[;:]\s*$', last_chunk_text):
            return False

        return True

    def get_semantic_overlap(self, current_chunk: list, overlap_size: int) -> list:
        """Obtenir un overlap s√©mantique plut√¥t que simplement les derniers mots."""
        if overlap_size == 0 or not current_chunk:
            return []

        # Chercher la derni√®re phrase compl√®te dans la limite de l'overlap
        chunk_text = ' '.join(current_chunk)
        sentences = re.split(r'(?<=[.!?])\s+', chunk_text)

        # Prendre la derni√®re phrase si elle fait moins que l'overlap
        if sentences and len(sentences[-1].split()) <= overlap_size:
            return sentences[-1].split()

        # Sinon, prendre les derniers mots
        return current_chunk[-overlap_size:] if len(current_chunk) >= overlap_size else current_chunk

    def create_chunk(self, content: str, chunk_id: int, user_id: str = None, project_id: str = None):
        """Cr√©er un chunk avec m√©tadonn√©es et analyse de qualit√©."""
        word_count = len(content.split())

        # Analyse de qualit√©
        quality_score = self.analyze_quality(content)

        # Extraction d'entit√©s
        entities = self.extract_entities(content)

        # Classification du contenu
        content_type = self.classify_content(content)

        # Structure simplifi√©e en 2 familles
        return {
            'content': {
                'text': content,
                'chunk_id': f'chunk_{chunk_id:03d}'
            },
            'metadata': {
                'word_count': word_count,
                'char_count': len(content),
                'position': chunk_id,
                'content_type': content_type,
                'quality_score': quality_score,
                'entities': entities,
                'hierarchical_title': self.get_title(content),
                'key_elements': self.extract_key_elements(content)
            },
            'userId': user_id,
            'projectId': project_id
        }

    def enrich_chunk_with_metadata(self, chunk: Dict, document_metadata: Dict) -> Dict:
        """Enrichir un chunk avec les m√©tadonn√©es du document."""
        # G√©n√©rer le format de source professionnel
        source_info = self.generate_professional_source(document_metadata, chunk['metadata'])

        # Structure finale avec 2 familles : content + document_info
        return {
            'content': chunk['content'],  # Texte + chunk_id
            'document_info': document_metadata,  # Toutes les m√©tadonn√©es du document
            'chunk_metadata': chunk['metadata'],  # M√©tadonn√©es sp√©cifiques au chunk
            'source_reference': source_info  # R√©f√©rence format√©e pour RAG
        }

    def generate_professional_source(self, document_metadata: Dict, chunk_metadata: Dict) -> str:
        """G√©n√©rer une r√©f√©rence source professionnelle."""
        # Construire le titre avec projet si disponible
        title = document_metadata.get('title', 'DOCUMENT JURIDIQUE')
        project = document_metadata.get('project', '')
        if project and project not in title:
            title = f"{title} - {project}"

        # Date format√©e
        date = document_metadata.get('date', '')
        date_str = f" ({date})" if date else ""

        # Parties format√©es
        parties = document_metadata.get('parties', {})
        parties_str = ""
        if parties:
            party_names = []
            for party_type, party_name in parties.items():
                if party_name:
                    party_names.append(party_name)
            if len(party_names) >= 2:
                parties_str = f"\nParties : {party_names[0]} vs {party_names[1]}"
            elif len(party_names) == 1:
                parties_str = f"\nPartie : {party_names[0]}"

        # Document ID
        doc_id = document_metadata.get('id', '')
        doc_id_str = f"\nDocument ID: {doc_id}" if doc_id else ""

        # Localisation si disponible
        location = document_metadata.get('location', '')
        location_str = f"\nLocalisation : {location}" if location else ""

        # Titre hi√©rarchique du chunk (clause, article, etc.)
        hierarchical_title = chunk_metadata.get('hierarchical_title', '')
        clause_str = f"\n{hierarchical_title}" if hierarchical_title and hierarchical_title != "Clause contractuelle" else ""

        # Assembler la r√©f√©rence compl√®te
        source_reference = f"Source : {title}{date_str}{parties_str}{doc_id_str}{location_str}{clause_str}"

        return source_reference.strip()

    def analyze_quality(self, content: str) -> float:
        """Analyse de qualit√© avanc√©e bas√©e sur de multiples facteurs."""
        words = content.split()
        word_count = len(words)

        # 1. Facteur longueur optimis√© (courbe gaussienne)
        optimal_length = 55  # Taille optimale pour l'immobilier
        length_variance = 25
        length_factor = max(0.3, 1.0 - ((word_count - optimal_length) ** 2) / (2 * length_variance ** 2))

        # 2. Facteur mots-cl√©s immobiliers pond√©r√©s
        weighted_keywords = {
            # Mots-cl√©s prioritaires (poids 3)
            'contrat': 3, 'prix': 3, 'd√©lai': 3, 'garantie': 3, 'obligation': 3,
            # Mots-cl√©s importants (poids 2)
            'article': 2, 'clause': 2, 'conditions': 2, 'responsabilit√©': 2,
            'livraison': 2, 'paiement': 2, 'travaux': 2, 'entreprise': 2,
            # Mots-cl√©s secondaires (poids 1)
            'partie': 1, 'engagement': 1, 'modalit√©': 1, '√©ch√©ance': 1,
            'conformit√©': 1, 'ex√©cution': 1, 'r√©ception': 1, 'achevements': 1
        }

        keyword_score = sum(weighted_keywords.get(word.lower(), 0) for word in words)
        max_possible_score = word_count * 3  # Si tous les mots √©taient prioritaires
        keyword_factor = min(1.0, keyword_score / max(max_possible_score * 0.2, 1))

        # 3. Facteur entit√©s enrichi
        entities = self.extract_entities(content)
        entity_score = 0

        # Dates (important pour les d√©lais)
        if entities['dates']:
            entity_score += 0.15
        if entities['deadlines']:
            entity_score += 0.1

        # Montants financiers (critiques)
        if entities['monetary_amounts']:
            entity_score += 0.2

        # R√©f√©rences l√©gales (fond juridique)
        if entities['legal_references']:
            entity_score += 0.15

        # Mesures et sp√©cifications (technique)
        if entities['measurements'] or entities['technical_specs']:
            entity_score += 0.1

        # Acteurs et assurances (contextuel)
        if entities['real_estate_actors'] or entities['insurance_terms']:
            entity_score += 0.1

        # Normes (qualit√© technique)
        if entities['norms_standards']:
            entity_score += 0.1

        entity_factor = min(1.0, 0.4 + entity_score)  # Base 0.4 + bonus

        # 4. Facteur structure avanc√©
        complete_sentences = len(re.findall(r'[.!?]', content))

        # Bonus pour la structure logique
        has_enumeration = bool(re.search(r'[;:]\s*(?:\n|$)', content))
        has_paragraphs = '\n' in content.strip()
        has_proper_punctuation = complete_sentences > 0

        structure_base = min(1.0, complete_sentences / 2)
        structure_bonus = 0.1 * has_enumeration + 0.1 * has_paragraphs + 0.1 * has_proper_punctuation
        structure_factor = min(1.0, structure_base + structure_bonus)

        # 5. Facteur coh√©sion s√©mantique
        coherence_factor = self.calculate_semantic_coherence(content, words)

        # 6. Facteur sp√©cificit√© immobilier
        real_estate_terms = [
            'construction', 'b√¢timent', 'immeuble', 'logement', 'terrain',
            'permis', 'urbanisme', 'ma√Ætre d\'ouvrage', 'architecte',
            'entrepreneur', 'chantier', 'r√©ception', 'livraison', 'conformit√©'
        ]
        re_term_count = sum(1 for term in real_estate_terms if term in content.lower())
        specificity_factor = min(1.0, 0.7 + (re_term_count / len(real_estate_terms)) * 0.3)

        # Score final pond√©r√© optimis√©
        score = (
            0.25 * length_factor +      # Longueur optimale
            0.20 * keyword_factor +     # Mots-cl√©s juridiques
            0.25 * entity_factor +      # Richesse des entit√©s
            0.15 * structure_factor +   # Structure du texte
            0.10 * coherence_factor +   # Coh√©sion s√©mantique
            0.05 * specificity_factor   # Sp√©cificit√© immobilier
        )

        return round(min(1.0, score), 3)

    def calculate_semantic_coherence(self, content: str, words: list) -> float:
        """Calculer la coh√©sion s√©mantique du contenu."""
        if len(words) < 10:
            return 0.5

        # D√©tecter la r√©p√©tition excessive
        unique_words = set(word.lower() for word in words if len(word) > 3)
        repetition_ratio = len(unique_words) / len([w for w in words if len(w) > 3])
        repetition_factor = min(1.0, repetition_ratio * 2)  # P√©naliser la r√©p√©tition

        # D√©tecter les connecteurs logiques
        connectors = [
            'et', 'ou', 'mais', 'donc', 'car', 'ainsi', 'alors', 'cependant',
            'toutefois', 'n√©anmoins', 'par cons√©quent', 'en effet', 'de plus',
            'en outre', 'notamment', 'c\'est-√†-dire', 'autrement dit'
        ]
        connector_count = sum(1 for conn in connectors if conn in content.lower())
        connector_factor = min(1.0, 0.6 + (connector_count / len(content.split())) * 10)

        # D√©tecter la continuit√© th√©matique
        theme_consistency = self.calculate_theme_consistency(content)

        return round((repetition_factor + connector_factor + theme_consistency) / 3, 3)

    def calculate_theme_consistency(self, content: str) -> float:
        """Calculer la consistance th√©matique."""
        content_lower = content.lower()

        # D√©finir les th√®mes principaux
        themes = {
            'contractuel': ['contrat', 'clause', 'obligation', 'engagement', 'accord'],
            'financier': ['prix', 'co√ªt', 'montant', 'paiement', 'facture', 'devis'],
            'temporel': ['d√©lai', 'livraison', '√©ch√©ance', 'planning', 'dur√©e'],
            'technique': ['travaux', 'construction', 'mat√©riaux', 'norme', 'qualit√©'],
            'juridique': ['responsabilit√©', 'garantie', 'assurance', 'droit', 'loi']
        }

        theme_scores = {}
        for theme_name, keywords in themes.items():
            score = sum(1 for keyword in keywords if keyword in content_lower)
            if score > 0:
                theme_scores[theme_name] = score

        if not theme_scores:
            return 0.5

        # Calculer la dominance du th√®me principal
        max_score = max(theme_scores.values())
        total_score = sum(theme_scores.values())

        # Un contenu coh√©rent a un th√®me dominant mais pas exclusif
        theme_dominance = max_score / total_score if total_score > 0 else 0

        # Optimum autour de 0.4-0.6 (pas trop dispers√©, pas trop mono-th√©matique)
        if 0.3 <= theme_dominance <= 0.7:
            return min(1.0, 0.8 + (1 - abs(theme_dominance - 0.5)) * 0.4)
        else:
            return max(0.3, 0.8 - abs(theme_dominance - 0.5))

    def extract_entities(self, content: str) -> Dict:
        """Extraction d'entit√©s juridiques et techniques avanc√©e."""
        entities = {
            'dates': [],
            'monetary_amounts': [],
            'legal_references': [],
            'measurements': [],
            'norms_standards': [],
            'materials': [],
            'technical_specs': [],
            'real_estate_actors': [],
            'insurance_terms': [],
            'deadlines': [],
            'penalties': []
        }

        # Dates (formats fran√ßais √©tendus)
        date_patterns = [
            r'\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{2,4}',
            r'\d{1,2}\s+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+\d{4}',
            r'(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+\d{4}',
            r'\d{1,2}er?\s+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+\d{4}',
            r'(?:lundi|mardi|mercredi|jeudi|vendredi|samedi|dimanche)\s+\d{1,2}'
        ]
        for pattern in date_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['dates'].extend(matches)

        # Montants et devises avanc√©s
        amount_patterns = [
            r'\d+[\s,]*\d*[\.,]\d{2}\s*(?:euros?|‚Ç¨)',
            r'\d+[\s,]*\d*\s*(?:euros?|‚Ç¨)',
            r'\d+[\s,]*\d*\s*EUR',
            r'\d+[\s,]*\d*\s*(?:\$|dollars?)',
            r'(?:prix|co√ªt|montant|tarif)\s*:?\s*\d+[\s,]*\d*',
            r'\d+[\s,]*\d*\s*(?:k‚Ç¨|K‚Ç¨|milliers?\s*d\'euros?)',
            r'TVA\s*:?\s*\d+[\.,]?\d*\s*%?',
            r'HT|TTC'
        ]
        for pattern in amount_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['monetary_amounts'].extend(matches)

        # R√©f√©rences l√©gales √©tendues
        legal_patterns = [
            r'article\s+[a-z]?\d+[\-\d]*(?:\s*bis|ter|quater)?',
            r'[Ll]\s*\d+[\-\d]*',
            r'[Rr]\s*\d+[\-\d]*',
            r'd√©cret\s+n¬∞?\s*[\d\-]+',
            r'loi\s+n¬∞?\s*[\d\-]+',
            r'code\s+(?:civil|p√©nal|de\s+(?:la\s+)?construction|du\s+travail|de\s+l\'urbanisme)',
            r'CCH|CGCT|CPC|CPP',
            r'arr√™t√©\s+(?:du\s+)?\d+',
            r'circulaire\s+n¬∞?\s*[\d\-]+'
        ]
        for pattern in legal_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['legal_references'].extend(matches)

        # Mesures et sp√©cifications techniques √©tendues
        measure_patterns = [
            r'\d+[\.,]?\d*\s*(?:m¬≤|m2|m√®tres?\s*carr√©s?)',
            r'\d+[\.,]?\d*\s*(?:m¬≥|m3|m√®tres?\s*cubes?)',
            r'\d+[\.,]?\d*\s*(?:ml?|m√®tres?\s*lin√©aires?)',
            r'\d+[\.,]?\d*\s*(?:cm|centim√®tres?)',
            r'\d+[\.,]?\d*\s*(?:mm|millim√®tres?)',
            r'\d+[\.,]?\d*\s*(?:kg|kilogrammes?)',
            r'\d+[\.,]?\d*\s*(?:tonnes?)',
            r'\d+[\.,]?\d*\s*(?:%|pour\s*cent)',
            r'\d+[\.,]?\d*\s*(?:kW|MW|watts?)',
            r'\d+[\.,]?\d*\s*(?:¬∞C|degr√©s?)',
            r'√©paisseur\s*:?\s*\d+[\.,]?\d*\s*(?:cm|mm)',
            r'hauteur\s*:?\s*\d+[\.,]?\d*\s*m'
        ]
        for pattern in measure_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['measurements'].extend(matches)

        # Normes et standards BTP √©tendus
        norm_patterns = [
            r'DTU\s+[\d\.]+',
            r'NF\s+EN\s+[\d\-]+',
            r'NF\s+[ABCP]\d+[\-\d]*',
            r'ISO\s+[\d\-]+',
            r'EN\s+[\d\-]+',
            r'AFNOR\s+[\w\d\-]+',
            r'RT\s*20(?:05|12|20)',
            r'RE\s*2020',
            r'BBC|HQE|BEPOS|HPE',
            r'Effinergie|Passivhaus',
            r'QUALIBAT|RGE|OPQIBI'
        ]
        for pattern in norm_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['norms_standards'].extend(matches)

        # Mat√©riaux BTP √©tendus
        materials_extended = [
            'b√©ton', 'acier', 'bois', 'pl√¢tre', 'ciment', 'sable', 'gravier',
            'parpaing', 'brique', 'tuile', 'ardoise', 'zinc', 'cuivre',
            'aluminium', 'PVC', 'polystyr√®ne', 'laine de verre', 'laine de roche',
            'isolant', 'membrane', '√©tanch√©it√©', 'rev√™tement', 'enduit',
            'carrelage', 'parquet', 'moquette', 'peinture', 'cr√©pi',
            'charpente', 'poutre', 'poteau', 'dalle', 'cloison'
        ]
        for material in materials_extended:
            if material in content.lower():
                entities['materials'].append(material)

        # Acteurs immobiliers
        real_estate_patterns = [
            r'ma√Ætre\s+d\'ouvrage',
            r'ma√Ætre\s+d\'≈ìuvre',
            r'architecte',
            r'entrepreneur',
            r'sous-traitant',
            r'bureau\s+d\'√©tudes',
            r'contr√¥leur\s+technique',
            r'coordonnateur\s+sps',
            r'g√©om√®tre',
            r'notaire',
            r'promoteur',
            r'lotisseur'
        ]
        for pattern in real_estate_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['real_estate_actors'].extend(matches)

        # Termes d'assurance
        insurance_patterns = [
            r'dommages?[-\s]ouvrage',
            r'd√©cennale',
            r'biennale',
            r'parfait\s+ach√®vement',
            r'responsabilit√©\s+civile',
            r'tous\s+risques?\s+chantier',
            r'garantie\s+financi√®re',
            r'caution\s+de\s+soumission'
        ]
        for pattern in insurance_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['insurance_terms'].extend(matches)

        # D√©lais sp√©cifiques
        deadline_patterns = [
            r'\d+\s*jours?\s*(?:ouvrables?|ouvr√©s?|calendaires?)?',
            r'\d+\s*semaines?',
            r'\d+\s*mois',
            r'\d+\s*ann√©es?',
            r'd√©lai\s+de\s+\d+',
            r'dans\s+les\s+\d+\s*(?:jours?|mois)',
            r'avant\s+le\s+\d+',
            r'√†\s+compter\s+du?'
        ]
        for pattern in deadline_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['deadlines'].extend(matches)

        # P√©nalit√©s et sanctions
        penalty_patterns = [
            r'p√©nalit√©s?\s*(?:de\s+retard)?',
            r'astreintes?',
            r'dommages?[-\s]int√©r√™ts',
            r'indemnit√©s?',
            r'r√©solution\s+du\s+contrat',
            r'r√©siliation',
            r'exclusion',
            r'\d+[\.,]?\d*\s*%\s*par\s*(?:jour|mois)\s*de\s*retard'
        ]
        for pattern in penalty_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['penalties'].extend(matches)

        # Sp√©cifications techniques
        tech_spec_patterns = [
            r'classe\s+[A-F]\d*',
            r'r√©sistance\s*:?\s*\d+[\.,]?\d*\s*MPa',
            r'pH\s*:?\s*\d+[\.,]?\d*',
            r'temp√©rature\s*:?\s*[\-\+]?\d+[\.,]?\d*\s*¬∞C',
            r'pression\s*:?\s*\d+[\.,]?\d*\s*bars?',
            r'viscosit√©\s*:?\s*\d+',
            r'granulom√©trie\s*:?\s*\d+[\.,]?\d*'
        ]
        for pattern in tech_spec_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['technical_specs'].extend(matches)

        return entities

    def classify_content(self, content: str) -> str:
        """Classification avanc√©e du type de contenu."""
        content_lower = content.lower()

        # Cat√©gories enrichies avec pond√©ration
        weighted_categories = {
            'financial': {
                'keywords': ['prix', 'co√ªt', 'tarif', 'montant', 'euros', '‚Ç¨', 'facture', 'paiement', 'acompte', 'devis', 'budget', 'tva', 'ht', 'ttc'],
                'patterns': [r'\d+[\s,]*\d*\s*(?:euros?|‚Ç¨)', r'\d+[\s,]*\d*\s*%', r'tva\s*:?\s*\d+'],
                'weight': 1.2
            },
            'timeline': {
                'keywords': ['d√©lai', 'livraison', '√©ch√©ance', 'date', 'planning', 'dur√©e', 'terme', 'calendrier', 'programmation'],
                'patterns': [r'\d+\s*(?:jours?|mois|semaines?)', r'avant\s+le', r'√†\s+compter\s+du?'],
                'weight': 1.1
            },
            'obligations': {
                'keywords': ['obligation', 'engage', 'doit', 'responsabilit√©', 'devoir', 'tenu', 'charge', 'incombe'],
                'patterns': [r'(?:doit|devra|s\'engage)\s+√†?', r'est\s+tenu\s+de'],
                'weight': 1.3
            },
            'guarantees': {
                'keywords': ['garantie', 'assurance', 'caution', 'couverture', 'protection', 'd√©cennale', 'dommages-ouvrage'],
                'patterns': [r'garantie\s+de', r'assurance\s+\w+', r'caution\s+de'],
                'weight': 1.2
            },
            'technical_requirements': {
                'keywords': ['technique', 'norme', 'sp√©cification', 'mat√©riau', 'DTU', 'performance', 'qualit√©', 'conformit√©', 'r√©sistance'],
                'patterns': [r'DTU\s+[\d\.]+', r'NF\s+EN', r'conforme\s+√†'],
                'weight': 1.1
            },
            'conditions': {
                'keywords': ['condition', 'clause', 'modalit√©', 'stipulation', 'disposition', 'si', 'sous\sr√©serve', 'sauf'],
                'patterns': [r'sous\s+(?:condition|r√©serve)', r'si\s+\w+', r'sauf\s+\w+'],
                'weight': 1.0
            },
            'quality_control': {
                'keywords': ['contr√¥le', 'v√©rification', 'test', 'essai', 'conformit√©', 'inspection', 'validation'],
                'patterns': [r'contr√¥le\s+de', r'v√©rification\s+de', r'conforme\s+aux?'],
                'weight': 1.0
            },
            'safety_security': {
                'keywords': ['s√©curit√©', 'protection', 'risque', 'danger', 'pr√©vention', 'accident', 'sps', 'epi'],
                'patterns': [r'plan\s+de\s+pr√©vention', r'mesures?\s+de\s+s√©curit√©'],
                'weight': 1.0
            },
            'administrative': {
                'keywords': ['autorisation', 'permis', 'd√©claration', 'formalit√©', 'proc√©dure', 'dossier'],
                'patterns': [r'permis\s+de', r'autorisation\s+de', r'd√©claration\s+de'],
                'weight': 0.9
            },
            'insurance': {
                'keywords': ['assurance', 'assureur', 'sinistre', 'indemnisation', 'prime', 'franchise'],
                'patterns': [r'police\s+d\'assurance', r'contrat\s+d\'assurance'],
                'weight': 1.1
            },
            'penalties': {
                'keywords': ['p√©nalit√©', 'amende', 'sanction', 'dommages-int√©r√™ts', 'astreinte', 'indemnit√©'],
                'patterns': [r'p√©nalit√©s?\s+de', r'\d+[\.,]?\d*\s*%\s*par'],
                'weight': 1.2
            }
        }

        scores = {}
        for category, data in weighted_categories.items():
            # Score des mots-cl√©s
            keyword_score = sum(1 for keyword in data['keywords'] if keyword in content_lower)

            # Score des patterns
            pattern_score = sum(len(re.findall(pattern, content_lower)) for pattern in data.get('patterns', []))

            # Score pond√©r√© final
            total_score = (keyword_score + pattern_score * 1.5) * data['weight']

            if total_score > 0:
                scores[category] = total_score

        if scores:
            # Seuil minimum pour √©viter les fausses classifications
            max_score = max(scores.values())
            if max_score >= 1.5:  # Seuil de confiance
                return max(scores, key=scores.get)

        return 'general'

    def get_classification_scores(self, content: str) -> Dict:
        """Obtenir les scores de classification pour tous les types."""
        content_lower = content.lower()

        categories = {
            'obligations': ['obligation', 'engage', 'doit', 'responsabilit√©'],
            'conditions': ['condition', 'clause', 'modalit√©', 'si'],
            'financial': ['prix', 'co√ªt', 'euros', '‚Ç¨', 'montant'],
            'timeline': ['d√©lai', 'date', '√©ch√©ance', 'livraison'],
            'guarantees': ['garantie', 'assurance', 'caution'],
            'technical_requirements': ['technique', 'norme', 'sp√©cification'],
            'quality_control': ['contr√¥le', 'v√©rification', 'test'],
            'safety_security': ['s√©curit√©', 'protection', 'risque'],
            'administrative': ['autorisation', 'permis', 'd√©claration'],
            'definitions': ['d√©finition', 'signifie', 'd√©signe'],
            'procedures': ['proc√©dure', 'm√©thode', '√©tape']
        }

        scores = {}
        for category, keywords in categories.items():
            scores[category] = sum(1 for keyword in keywords if keyword in content_lower)

        return scores

    def calculate_coherence(self, content: str) -> float:
        """Calculer la coh√©rence du contenu."""
        sentences = re.split(r'[.!?]+', content)
        if len(sentences) < 2:
            return 0.8

        # V√©rifier la pr√©sence de connecteurs
        connectors = ['et', 'ou', 'mais', 'donc', 'car', 'ainsi', 'alors', 'cependant', 'toutefois']
        connector_count = sum(1 for conn in connectors if conn in content.lower())

        coherence = min(1.0, 0.6 + (connector_count / len(sentences)) * 0.4)
        return round(coherence, 3)

    def calculate_relevance(self, content: str) -> float:
        """Calculer la pertinence juridique."""
        legal_terms = [
            'contrat', 'accord', 'convention', 'engagement', 'obligation',
            'droit', 'devoir', 'clause', 'article', 'disposition'
        ]

        words = content.lower().split()
        legal_word_count = sum(1 for word in words if word in legal_terms)

        relevance = min(1.0, 0.7 + (legal_word_count / len(words)) * 3)
        return round(relevance, 3)

    def calculate_factual_density(self, entities: Dict) -> float:
        """Calculer la densit√© factuelle bas√©e sur les entit√©s."""
        total_entities = sum(len(entity_list) for entity_list in entities.values())
        # Normaliser par rapport √† un chunk moyen
        density = min(1.0, total_entities / 5)
        return round(density, 3)

    def get_title(self, content: str) -> str:
        """Extraire ou g√©n√©rer un titre pour le chunk."""
        # Chercher un article num√©rot√©
        article_match = re.search(r'article\s+\d+[^\n.]*', content, re.IGNORECASE)
        if article_match:
            return article_match.group(0).strip()

        # Chercher une clause
        clause_match = re.search(r'clause\s+[^\n.]*', content, re.IGNORECASE)
        if clause_match:
            return clause_match.group(0).strip()

        # Premi√®re phrase courte
        sentences = re.split(r'[.!?]+', content)
        if sentences and len(sentences[0]) < 80:
            return sentences[0].strip()

        return "Clause contractuelle"

    def extract_key_elements(self, content: str) -> list:
        """Extraire les √©l√©ments cl√©s du chunk."""
        elements = []

        # Mots-cl√©s importants avec priorit√©
        priority_words = {
            'contrat': 3, 'prix': 3, 'd√©lai': 3, 'garantie': 3,
            'obligation': 2, 'clause': 2, 'article': 2, 'conditions': 2,
            'paiement': 2, 'livraison': 2, 'responsabilit√©': 2,
            'assurance': 1, 'modalit√©': 1, '√©ch√©ance': 1
        }

        content_lower = content.lower()
        word_scores = []

        for word, priority in priority_words.items():
            if word in content_lower:
                word_scores.append((word, priority))

        # Trier par priorit√© et prendre les 5 premiers
        word_scores.sort(key=lambda x: x[1], reverse=True)
        elements = [word for word, _ in word_scores[:5]]

        return elements

    def detect_document_type(self, text: str) -> str:
        """D√©tection du type de document."""
        text_lower = text.lower()

        patterns = {
            'contrat_reservation_vefa': ['vefa', 'r√©servation', 'futur ach√®vement'],
            'cctp': ['cctp', 'clauses techniques particuli√®res', 'cahier des charges'],
            'acte_notarie': ['acte', 'notaire', 'notari√©', '√©tude'],
            'bail_habitation': ['bail', 'location', 'logement', 'loyer'],
            'bail_commercial': ['bail commercial', 'fonds de commerce'],
            'marche_public': ['march√© public', 'appel d\'offres', 'soumission'],
            'permis_construire': ['permis de construire', 'autorisation', 'urbanisme'],
            'devis': ['devis', 'estimation', 'chiffrage'],
            'facture': ['facture', 'facturation', 'tva']
        }

        for doc_type, keywords in patterns.items():
            if any(keyword in text_lower for keyword in keywords):
                return doc_type

        return 'contrat_general'

# Instance du service avec initialisation
chunking_service = ChunkingService()

@app.get("/")
async def root():
    """Point d'entr√©e principal de l'API."""
    return {
        "message": "Legal Document Chunking API - Version Pro avec m√©tadonn√©es contextuelles",
        "version": "2.1.0",
        "features": [
            "Extraction automatique des m√©tadonn√©es documentaires",
            "G√©n√©ration d'ID standardis√©s",
            "Structure JSON optimis√©e pour RAG",
            "R√©f√©rences sources professionnelles",
            "Support multi-types : VEFA, CCTP, Baux, etc."
        ],
        "endpoints": {
            "chunk": "/chunk - POST - Chunking de documents avec m√©tadonn√©es",
            "health": "/health - GET - Status de l'API",
            "docs": "/docs - GET - Documentation interactive"
        }
    }

@app.get("/health")
async def health():
    """Endpoint de sant√© pour v√©rification."""
    return {
        "status": "healthy",
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

@app.post("/chunk")
async def chunk_document(request: ChunkingRequest):
    """Endpoint principal pour le chunking de documents."""
    try:
        extracted_text = request.extractedText.strip()

        if not extracted_text:
            raise HTTPException(
                status_code=400,
                detail="Le champ extractedText est requis et ne peut pas √™tre vide"
            )

        if len(extracted_text) < 100:
            raise HTTPException(
                status_code=400,
                detail="Le texte doit contenir au moins 100 caract√®res"
            )

        # Options avec valeurs par d√©faut
        options = request.options
        target_chunk_size = options.get('target_chunk_size', 60)
        overlap_size = options.get('overlap_size', 15)

        # Validation des param√®tres
        if not (20 <= target_chunk_size <= 200):
            raise HTTPException(
                status_code=400,
                detail="target_chunk_size doit √™tre entre 20 et 200 mots"
            )

        if not (0 <= overlap_size <= 50):
            raise HTTPException(
                status_code=400,
                detail="overlap_size doit √™tre entre 0 et 50 mots"
            )

        # Traitement du chunking
        start_time = time.time()

        chunks = chunking_service.create_smart_chunks(
            extracted_text,
            target_chunk_size,
            overlap_size,
            user_id=request.userId,
            project_id=request.projectId
        )

        end_time = time.time()
        processing_time = int((end_time - start_time) * 1000)

        # Calcul des statistiques sur la nouvelle structure
        quality_dist = {'high': 0, 'medium': 0, 'low': 0}
        total_quality = 0
        avg_chunk_size = 0

        if chunks:
            for chunk in chunks:
                score = chunk['chunk_metadata']['quality_score']
                total_quality += score
                avg_chunk_size += chunk['chunk_metadata']['word_count']

                if score >= 0.8:
                    quality_dist['high'] += 1
                elif score >= 0.5:
                    quality_dist['medium'] += 1
                else:
                    quality_dist['low'] += 1

            avg_chunk_size = avg_chunk_size / len(chunks)

        avg_quality = total_quality / len(chunks) if chunks else 0

        # Validation des r√©sultats
        low_quality_rate = (quality_dist['low'] / len(chunks) * 100) if chunks else 0

        validation_results = {
            'consistency_score': 0.85,
            'cross_reference_integrity': 0.90,
            'missing_elements': [],
            'recommendations': []
        }

        if low_quality_rate > 30:
            validation_results['recommendations'].append(
                "Taux √©lev√© de chunks de basse qualit√© - consid√©rer l'augmentation de target_chunk_size"
            )

        # R√©ponse finale avec nouvelle structure
        response_data = {
            'success': True,
            'chunks': chunks,
            'document_stats': {
                'document_info': chunks[0]['document_info'] if chunks else {},
                'estimated_complexity': 'medium',
                'processing_time_ms': processing_time,
                'total_chunks': len(chunks),
                'avg_chunk_quality': round(avg_quality, 3),
                'quality_distribution': quality_dist,
                'text_length': len(extracted_text),
                'avg_chunk_size': avg_chunk_size
            },
            'validation_results': validation_results,
            'processing_info': {
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'version': '2.1.0',
                'target_chunk_size': target_chunk_size,
                'overlap_size': overlap_size
            }
        }

        return JSONResponse(content=response_data)

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Erreur interne du serveur: {str(e)}"
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)