"""
FastAPI server for Legal Document Chunking
Designed for VPS deployment with n8n
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Optional, Dict, Any
import json
import re
import time
from datetime import datetime, timezone

app = FastAPI(
    title="Legal Document Chunking API",
    description="API pour le chunking intelligent de documents juridiques fran√ßais",
    version="2.0.0"
)

# Force OpenAPI 3.0.2 pour compatibilit√© avec Swagger UI
def custom_openapi():
    if app.openapi_schema:
        return app.openapi_schema
    from fastapi.openapi.utils import get_openapi
    openapi_schema = get_openapi(
        title=app.title,
        version=app.version,
        description=app.description,
        routes=app.routes,
        openapi_version="3.0.2"
    )
    app.openapi_schema = openapi_schema
    return app.openapi_schema

app.openapi = custom_openapi

# CORS pour n8n
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChunkingRequest(BaseModel):
    extractedText: str
    options: Optional[Dict[str, Any]] = {}

class ChunkingService:
    """Service de chunking intelligent pour documents juridiques - Version Pro."""

    def __init__(self):
        # Dictionnaires sp√©cialis√©s pour l'immobilier
        self.real_estate_terms = {
            'construction': ['gros ≈ìuvre', 'second ≈ìuvre', 'fondations', 'charpente', 'couverture', '√©tanch√©it√©', 'isolation', 'cloisons', 'rev√™tements'],
            'legal_actors': ['ma√Ætre d\'ouvrage', 'ma√Ætre d\'≈ìuvre', 'architecte', 'entrepreneur', 'sous-traitant', 'bureau d\'√©tudes', 'contr√¥leur technique'],
            'insurance': ['dommages-ouvrage', 'd√©cennale', 'biennale', 'parfait ach√®vement', 'responsabilit√© civile', 'tous risques chantier'],
            'certifications': ['RT2012', 'RT2020', 'HQE', 'BBC', 'BEPOS', 'NF Habitat', 'Qualitel'],
            'procedures': ['permis de construire', 'd√©claration pr√©alable', 'DT-DICT', 'r√©ception des travaux', 'lev√©e des r√©serves'],
            'financial_terms': ['r√©vision de prix', 'actualisation', 'variation', 'acompte', 'avancement', 'retenue de garantie', 'd√©compte']
        }

    def create_smart_chunks(self, text: str, target_size: int = 60, overlap: int = 15):
        """Cr√©er des chunks intelligents avec gestion avanc√©e des structures."""
        # Nettoyage et pr√©paration du texte
        text = self.preprocess_text(text)

        # D√©tection des structures sp√©ciales
        special_sections = self.detect_special_structures(text)

        # Adaptation de la taille selon le contenu
        adaptive_size = self.calculate_adaptive_size(text, target_size)

        # Traitement selon la structure d√©tect√©e
        if special_sections['has_tables']:
            return self.chunk_with_table_handling(text, adaptive_size, overlap)
        else:
            return self.chunk_standard_content(text, adaptive_size, overlap)

    def preprocess_text(self, text: str) -> str:
        """Pr√©traitement avanc√© du texte."""
        # Nettoyage des espaces multiples
        text = re.sub(r'\s+', ' ', text.strip())

        # Normalisation des caract√®res sp√©ciaux
        text = re.sub(r'[\u00A0\u2000-\u200B\u2028\u2029]', ' ', text)

        # Protection des structures importantes
        text = self.protect_legal_structures(text)

        return text

    def protect_legal_structures(self, text: str) -> str:
        """Prot√©ger les structures juridiques importantes."""
        # Prot√©ger les r√©f√©rences d'articles
        text = re.sub(r'(article\s+[a-z]?\d+[\-\d]*)', r'\1üîí', text, flags=re.IGNORECASE)

        # Prot√©ger les montants
        text = re.sub(r'(\d+[\s,]*\d*\s*(?:euros?|‚Ç¨))', r'\1üîí', text, flags=re.IGNORECASE)

        return text

    def detect_special_structures(self, text: str) -> dict:
        """D√©tecter les structures sp√©ciales dans le document."""
        return {
            'has_tables': '|' in text or 'Nom / D√©nomination' in text,
            'has_numbered_clauses': bool(re.search(r'^\s*\d+[\./)]', text, re.MULTILINE)),
            'has_legal_formulas': bool(re.search(r'\$[^\$]+\$', text)),
            'has_complex_references': len(re.findall(r'article\s+[a-z]?\d+', text, re.IGNORECASE)) > 5
        }

    def calculate_adaptive_size(self, text: str, base_size: int) -> int:
        """Calculer la taille adaptative selon le type de contenu."""
        content_type = self.classify_global_content(text)

        size_adjustments = {
            'financial': base_size + 10,  # Plus de contexte pour les clauses financi√®res
            'technical_requirements': base_size + 15,  # Sp√©cifications techniques d√©taill√©es
            'obligations': base_size - 5,  # Obligations plus concises
            'timeline': base_size,
            'legal_references': base_size - 10  # Articles courts et pr√©cis
        }

        return size_adjustments.get(content_type, base_size)

    def classify_global_content(self, text: str) -> str:
        """Classification globale du document."""
        text_lower = text.lower()

        # Calcul des scores pour chaque cat√©gorie
        category_scores = {
            'financial': len(re.findall(r'prix|co√ªt|euros?|‚Ç¨|montant|facture|paiement|devis', text_lower)),
            'technical_requirements': len(re.findall(r'technique|norme|sp√©cification|mat√©riau|dtu|performance|qualit√©', text_lower)),
            'obligations': len(re.findall(r'obligation|engage|doit|responsabilit√©|devoir|tenu', text_lower)),
            'timeline': len(re.findall(r'd√©lai|livraison|√©ch√©ance|date|planning|dur√©e', text_lower)),
            'legal_references': len(re.findall(r'article|d√©cret|loi|code|r√©f√©rence', text_lower))
        }

        return max(category_scores, key=category_scores.get) if any(category_scores.values()) else 'general'

    def chunk_with_table_handling(self, text: str, target_size: int, overlap: int):
        """Chunking sp√©cialis√© pour les documents avec tableaux."""
        chunks = []
        chunk_id = 1

        # S√©paration du contenu par sections
        sections = self.split_by_tables(text)

        for section in sections:
            if self.is_table_content(section):
                # Traitement sp√©cialis√© pour les tableaux
                table_chunks = self.chunk_table_content(section, chunk_id)
                chunks.extend(table_chunks)
                chunk_id += len(table_chunks)
            else:
                # Traitement standard pour le texte normal
                section_chunks = self.chunk_standard_content(section, target_size, overlap, start_id=chunk_id)
                chunks.extend(section_chunks)
                chunk_id += len(section_chunks)

        return chunks

    def split_by_tables(self, text: str) -> list:
        """S√©parer le texte en sections normales et tableaux."""
        # Pattern pour d√©tecter les d√©buts de tableaux
        table_pattern = r'(\|[^|]*\|[^|]*\|)'

        sections = []
        last_end = 0

        for match in re.finditer(table_pattern, text):
            # Ajouter le texte avant le tableau
            if match.start() > last_end:
                sections.append(text[last_end:match.start()])

            # Trouver la fin du tableau
            table_start = match.start()
            table_end = self.find_table_end(text, table_start)
            sections.append(text[table_start:table_end])
            last_end = table_end

        # Ajouter le texte restant
        if last_end < len(text):
            sections.append(text[last_end:])

        return [s.strip() for s in sections if s.strip()]

    def find_table_end(self, text: str, start: int) -> int:
        """Trouver la fin d'un tableau."""
        lines = text[start:].split('\n')
        end_line = 0

        for i, line in enumerate(lines):
            if '|' not in line and line.strip() and not re.match(r'^\s*[-:]+\s*$', line):
                break
            end_line = i + 1

        return start + len('\n'.join(lines[:end_line]))

    def is_table_content(self, section: str) -> bool:
        """V√©rifier si une section contient un tableau."""
        return '|' in section and section.count('|') >= 4

    def chunk_table_content(self, table_text: str, start_id: int) -> list:
        """Chunking sp√©cialis√© pour les tableaux."""
        chunks = []
        lines = table_text.strip().split('\n')

        # Grouper les lignes de tableau en chunks logiques
        current_group = []
        header_processed = False

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # Conserver l'en-t√™te avec chaque groupe
            if not header_processed and '|' in line and not re.match(r'^\s*[-:]+\s*$', line):
                header = line
                header_processed = True
                current_group.append(line)
            elif re.match(r'^\s*[-:]+\s*$', line):
                current_group.append(line)
            else:
                current_group.append(line)

                # Cr√©er un chunk tous les 3-4 lignes de donn√©es
                if len(current_group) >= 4:
                    chunk_content = '\n'.join(current_group)
                    chunk = self.create_chunk(chunk_content, start_id)
                    chunks.append(chunk)

                    # Red√©marrer avec l'en-t√™te
                    current_group = [header] if 'header' in locals() else []
                    start_id += 1

        # Chunk final s'il reste des donn√©es
        if current_group and len(current_group) > 1:
            chunk_content = '\n'.join(current_group)
            chunk = self.create_chunk(chunk_content, start_id)
            chunks.append(chunk)

        return chunks

    def chunk_standard_content(self, text: str, target_size: int, overlap: int, start_id: int = 1):
        """Chunking standard am√©lior√©."""
        # D√©coupage par phrases avec gestion des abr√©viations
        sentences = self.smart_sentence_split(text)
        chunks = []
        current_chunk = []
        current_word_count = 0
        chunk_id = start_id

        for sentence in sentences:
            if not sentence.strip():
                continue

            sentence_words = sentence.split()

            # V√©rification de la coh√©sion avant de couper
            if (current_word_count + len(sentence_words) > target_size and
                current_chunk and self.can_split_here(sentence, current_chunk)):

                # Cr√©er le chunk actuel
                chunk_content = ' '.join(current_chunk)
                chunk = self.create_chunk(chunk_content, chunk_id)
                chunks.append(chunk)

                # Pr√©parer le chunk suivant avec overlap s√©mantique
                overlap_words = self.get_semantic_overlap(current_chunk, overlap)
                current_chunk = overlap_words + sentence_words
                current_word_count = len(current_chunk)
                chunk_id += 1
            else:
                # Ajouter la phrase au chunk actuel
                current_chunk.extend(sentence_words)
                current_word_count += len(sentence_words)

        # Chunk final
        if current_chunk:
            chunk_content = ' '.join(current_chunk)
            chunk = self.create_chunk(chunk_content, chunk_id)
            chunks.append(chunk)

        return chunks

    def smart_sentence_split(self, text: str) -> list:
        """D√©coupage intelligent des phrases avec gestion des abr√©viations."""
        # Prot√©ger les abr√©viations courantes
        abbreviations = ['art.', 'etc.', 'cf.', 'p.ex.', 'c.-√†-d.', 'M.', 'Mme', 'Dr.', 'Me']

        protected_text = text
        for abbr in abbreviations:
            protected_text = protected_text.replace(abbr, abbr.replace('.', 'üîí'))

        # D√©coupage par phrases
        sentences = re.split(r'(?<=[.!?])\s+', protected_text)

        # Restaurer les points
        sentences = [s.replace('üîí', '.') for s in sentences]

        return sentences

    def can_split_here(self, next_sentence: str, current_chunk: list) -> bool:
        """V√©rifier si on peut couper √† cet endroit sans nuire √† la coh√©sion."""
        if not current_chunk:
            return True

        # Ne pas couper si la phrase suivante commence par une conjonction
        conjunctions = ['et', 'ou', 'mais', 'donc', 'car', 'ainsi', 'alors', 'cependant', 'toutefois', 'n√©anmoins']
        first_word = next_sentence.strip().split()[0].lower() if next_sentence.strip() else ''

        if first_word in conjunctions:
            return False

        # Ne pas couper au milieu d'une √©num√©ration
        last_chunk_text = ' '.join(current_chunk[-10:])  # Derniers mots
        if re.search(r'[;:]\s*$', last_chunk_text):
            return False

        return True

    def get_semantic_overlap(self, current_chunk: list, overlap_size: int) -> list:
        """Obtenir un overlap s√©mantique plut√¥t que simplement les derniers mots."""
        if overlap_size == 0 or not current_chunk:
            return []

        # Chercher la derni√®re phrase compl√®te dans la limite de l'overlap
        chunk_text = ' '.join(current_chunk)
        sentences = re.split(r'(?<=[.!?])\s+', chunk_text)

        # Prendre la derni√®re phrase si elle fait moins que l'overlap
        if sentences and len(sentences[-1].split()) <= overlap_size:
            return sentences[-1].split()

        # Sinon, prendre les derniers mots
        return current_chunk[-overlap_size:] if len(current_chunk) >= overlap_size else current_chunk

    def create_chunk(self, content: str, chunk_id: int):
        """Cr√©er un chunk avec m√©tadonn√©es et analyse de qualit√©."""
        word_count = len(content.split())

        # Analyse de qualit√©
        quality_score = self.analyze_quality(content)

        # Extraction d'entit√©s
        entities = self.extract_entities(content)

        # Classification du contenu
        content_type = self.classify_content(content)

        return {
            'id': f'chunk_{chunk_id:03d}',
            'content': content,
            'hierarchical_title': self.get_title(content),
            'content_type': content_type,
            'section_info': {
                'type': 'content',
                'number': chunk_id,
                'title': self.get_title(content)
            },
            'content_classification': {
                'type': content_type,
                'confidence': 0.85,
                'alternatives': [],
                'scores': self.get_classification_scores(content)
            },
            'metadata': {
                'word_count': word_count,
                'char_count': len(content),
                'position': chunk_id,
                'has_legal_references': bool(entities['legal_references']),
                'has_financial_info': bool(entities['monetary_amounts']),
                'has_dates': bool(entities['dates'])
            },
            'quality_analysis': {
                'overall_score': quality_score,
                'completeness': min(1.0, word_count / 50),
                'coherence': self.calculate_coherence(content),
                'relevance': self.calculate_relevance(content),
                'factual_density': self.calculate_factual_density(entities)
            },
            'extracted_entities': entities,
            'key_elements': self.extract_key_elements(content)
        }

    def analyze_quality(self, content: str) -> float:
        """Analyse de qualit√© avanc√©e bas√©e sur de multiples facteurs."""
        words = content.split()
        word_count = len(words)

        # 1. Facteur longueur optimis√© (courbe gaussienne)
        optimal_length = 55  # Taille optimale pour l'immobilier
        length_variance = 25
        length_factor = max(0.3, 1.0 - ((word_count - optimal_length) ** 2) / (2 * length_variance ** 2))

        # 2. Facteur mots-cl√©s immobiliers pond√©r√©s
        weighted_keywords = {
            # Mots-cl√©s prioritaires (poids 3)
            'contrat': 3, 'prix': 3, 'd√©lai': 3, 'garantie': 3, 'obligation': 3,
            # Mots-cl√©s importants (poids 2)
            'article': 2, 'clause': 2, 'conditions': 2, 'responsabilit√©': 2,
            'livraison': 2, 'paiement': 2, 'travaux': 2, 'entreprise': 2,
            # Mots-cl√©s secondaires (poids 1)
            'partie': 1, 'engagement': 1, 'modalit√©': 1, '√©ch√©ance': 1,
            'conformit√©': 1, 'ex√©cution': 1, 'r√©ception': 1, 'achevements': 1
        }

        keyword_score = sum(weighted_keywords.get(word.lower(), 0) for word in words)
        max_possible_score = word_count * 3  # Si tous les mots √©taient prioritaires
        keyword_factor = min(1.0, keyword_score / max(max_possible_score * 0.2, 1))

        # 3. Facteur entit√©s enrichi
        entities = self.extract_entities(content)
        entity_score = 0

        # Dates (important pour les d√©lais)
        if entities['dates']:
            entity_score += 0.15
        if entities['deadlines']:
            entity_score += 0.1

        # Montants financiers (critiques)
        if entities['monetary_amounts']:
            entity_score += 0.2

        # R√©f√©rences l√©gales (fond juridique)
        if entities['legal_references']:
            entity_score += 0.15

        # Mesures et sp√©cifications (technique)
        if entities['measurements'] or entities['technical_specs']:
            entity_score += 0.1

        # Acteurs et assurances (contextuel)
        if entities['real_estate_actors'] or entities['insurance_terms']:
            entity_score += 0.1

        # Normes (qualit√© technique)
        if entities['norms_standards']:
            entity_score += 0.1

        entity_factor = min(1.0, 0.4 + entity_score)  # Base 0.4 + bonus

        # 4. Facteur structure avanc√©
        complete_sentences = len(re.findall(r'[.!?]', content))

        # Bonus pour la structure logique
        has_enumeration = bool(re.search(r'[;:]\s*(?:\n|$)', content))
        has_paragraphs = '\n' in content.strip()
        has_proper_punctuation = complete_sentences > 0

        structure_base = min(1.0, complete_sentences / 2)
        structure_bonus = 0.1 * has_enumeration + 0.1 * has_paragraphs + 0.1 * has_proper_punctuation
        structure_factor = min(1.0, structure_base + structure_bonus)

        # 5. Facteur coh√©sion s√©mantique
        coherence_factor = self.calculate_semantic_coherence(content, words)

        # 6. Facteur sp√©cificit√© immobilier
        real_estate_terms = [
            'construction', 'b√¢timent', 'immeuble', 'logement', 'terrain',
            'permis', 'urbanisme', 'ma√Ætre d\'ouvrage', 'architecte',
            'entrepreneur', 'chantier', 'r√©ception', 'livraison', 'conformit√©'
        ]
        re_term_count = sum(1 for term in real_estate_terms if term in content.lower())
        specificity_factor = min(1.0, 0.7 + (re_term_count / len(real_estate_terms)) * 0.3)

        # Score final pond√©r√© optimis√©
        score = (
            0.25 * length_factor +      # Longueur optimale
            0.20 * keyword_factor +     # Mots-cl√©s juridiques
            0.25 * entity_factor +      # Richesse des entit√©s
            0.15 * structure_factor +   # Structure du texte
            0.10 * coherence_factor +   # Coh√©sion s√©mantique
            0.05 * specificity_factor   # Sp√©cificit√© immobilier
        )

        return round(min(1.0, score), 3)

    def calculate_semantic_coherence(self, content: str, words: list) -> float:
        """Calculer la coh√©sion s√©mantique du contenu."""
        if len(words) < 10:
            return 0.5

        # D√©tecter la r√©p√©tition excessive
        unique_words = set(word.lower() for word in words if len(word) > 3)
        repetition_ratio = len(unique_words) / len([w for w in words if len(w) > 3])
        repetition_factor = min(1.0, repetition_ratio * 2)  # P√©naliser la r√©p√©tition

        # D√©tecter les connecteurs logiques
        connectors = [
            'et', 'ou', 'mais', 'donc', 'car', 'ainsi', 'alors', 'cependant',
            'toutefois', 'n√©anmoins', 'par cons√©quent', 'en effet', 'de plus',
            'en outre', 'notamment', 'c\'est-√†-dire', 'autrement dit'
        ]
        connector_count = sum(1 for conn in connectors if conn in content.lower())
        connector_factor = min(1.0, 0.6 + (connector_count / len(content.split())) * 10)

        # D√©tecter la continuit√© th√©matique
        theme_consistency = self.calculate_theme_consistency(content)

        return round((repetition_factor + connector_factor + theme_consistency) / 3, 3)

    def calculate_theme_consistency(self, content: str) -> float:
        """Calculer la consistance th√©matique."""
        content_lower = content.lower()

        # D√©finir les th√®mes principaux
        themes = {
            'contractuel': ['contrat', 'clause', 'obligation', 'engagement', 'accord'],
            'financier': ['prix', 'co√ªt', 'montant', 'paiement', 'facture', 'devis'],
            'temporel': ['d√©lai', 'livraison', '√©ch√©ance', 'planning', 'dur√©e'],
            'technique': ['travaux', 'construction', 'mat√©riaux', 'norme', 'qualit√©'],
            'juridique': ['responsabilit√©', 'garantie', 'assurance', 'droit', 'loi']
        }

        theme_scores = {}
        for theme_name, keywords in themes.items():
            score = sum(1 for keyword in keywords if keyword in content_lower)
            if score > 0:
                theme_scores[theme_name] = score

        if not theme_scores:
            return 0.5

        # Calculer la dominance du th√®me principal
        max_score = max(theme_scores.values())
        total_score = sum(theme_scores.values())

        # Un contenu coh√©rent a un th√®me dominant mais pas exclusif
        theme_dominance = max_score / total_score if total_score > 0 else 0

        # Optimum autour de 0.4-0.6 (pas trop dispers√©, pas trop mono-th√©matique)
        if 0.3 <= theme_dominance <= 0.7:
            return min(1.0, 0.8 + (1 - abs(theme_dominance - 0.5)) * 0.4)
        else:
            return max(0.3, 0.8 - abs(theme_dominance - 0.5))

    def extract_entities(self, content: str) -> Dict:
        """Extraction d'entit√©s juridiques et techniques avanc√©e."""
        entities = {
            'dates': [],
            'monetary_amounts': [],
            'legal_references': [],
            'measurements': [],
            'norms_standards': [],
            'materials': [],
            'technical_specs': [],
            'real_estate_actors': [],
            'insurance_terms': [],
            'deadlines': [],
            'penalties': []
        }

        # Dates (formats fran√ßais √©tendus)
        date_patterns = [
            r'\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{2,4}',
            r'\d{1,2}\s+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+\d{4}',
            r'(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+\d{4}',
            r'\d{1,2}er?\s+(?:janvier|f√©vrier|mars|avril|mai|juin|juillet|ao√ªt|septembre|octobre|novembre|d√©cembre)\s+\d{4}',
            r'(?:lundi|mardi|mercredi|jeudi|vendredi|samedi|dimanche)\s+\d{1,2}'
        ]
        for pattern in date_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['dates'].extend(matches)

        # Montants et devises avanc√©s
        amount_patterns = [
            r'\d+[\s,]*\d*[\.,]\d{2}\s*(?:euros?|‚Ç¨)',
            r'\d+[\s,]*\d*\s*(?:euros?|‚Ç¨)',
            r'\d+[\s,]*\d*\s*EUR',
            r'\d+[\s,]*\d*\s*(?:\$|dollars?)',
            r'(?:prix|co√ªt|montant|tarif)\s*:?\s*\d+[\s,]*\d*',
            r'\d+[\s,]*\d*\s*(?:k‚Ç¨|K‚Ç¨|milliers?\s*d\'euros?)',
            r'TVA\s*:?\s*\d+[\.,]?\d*\s*%?',
            r'HT|TTC'
        ]
        for pattern in amount_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['monetary_amounts'].extend(matches)

        # R√©f√©rences l√©gales √©tendues
        legal_patterns = [
            r'article\s+[a-z]?\d+[\-\d]*(?:\s*bis|ter|quater)?',
            r'[Ll]\s*\d+[\-\d]*',
            r'[Rr]\s*\d+[\-\d]*',
            r'd√©cret\s+n¬∞?\s*[\d\-]+',
            r'loi\s+n¬∞?\s*[\d\-]+',
            r'code\s+(?:civil|p√©nal|de\s+(?:la\s+)?construction|du\s+travail|de\s+l\'urbanisme)',
            r'CCH|CGCT|CPC|CPP',
            r'arr√™t√©\s+(?:du\s+)?\d+',
            r'circulaire\s+n¬∞?\s*[\d\-]+'
        ]
        for pattern in legal_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['legal_references'].extend(matches)

        # Mesures et sp√©cifications techniques √©tendues
        measure_patterns = [
            r'\d+[\.,]?\d*\s*(?:m¬≤|m2|m√®tres?\s*carr√©s?)',
            r'\d+[\.,]?\d*\s*(?:m¬≥|m3|m√®tres?\s*cubes?)',
            r'\d+[\.,]?\d*\s*(?:ml?|m√®tres?\s*lin√©aires?)',
            r'\d+[\.,]?\d*\s*(?:cm|centim√®tres?)',
            r'\d+[\.,]?\d*\s*(?:mm|millim√®tres?)',
            r'\d+[\.,]?\d*\s*(?:kg|kilogrammes?)',
            r'\d+[\.,]?\d*\s*(?:tonnes?)',
            r'\d+[\.,]?\d*\s*(?:%|pour\s*cent)',
            r'\d+[\.,]?\d*\s*(?:kW|MW|watts?)',
            r'\d+[\.,]?\d*\s*(?:¬∞C|degr√©s?)',
            r'√©paisseur\s*:?\s*\d+[\.,]?\d*\s*(?:cm|mm)',
            r'hauteur\s*:?\s*\d+[\.,]?\d*\s*m'
        ]
        for pattern in measure_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['measurements'].extend(matches)

        # Normes et standards BTP √©tendus
        norm_patterns = [
            r'DTU\s+[\d\.]+',
            r'NF\s+EN\s+[\d\-]+',
            r'NF\s+[ABCP]\d+[\-\d]*',
            r'ISO\s+[\d\-]+',
            r'EN\s+[\d\-]+',
            r'AFNOR\s+[\w\d\-]+',
            r'RT\s*20(?:05|12|20)',
            r'RE\s*2020',
            r'BBC|HQE|BEPOS|HPE',
            r'Effinergie|Passivhaus',
            r'QUALIBAT|RGE|OPQIBI'
        ]
        for pattern in norm_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['norms_standards'].extend(matches)

        # Mat√©riaux BTP √©tendus
        materials_extended = [
            'b√©ton', 'acier', 'bois', 'pl√¢tre', 'ciment', 'sable', 'gravier',
            'parpaing', 'brique', 'tuile', 'ardoise', 'zinc', 'cuivre',
            'aluminium', 'PVC', 'polystyr√®ne', 'laine de verre', 'laine de roche',
            'isolant', 'membrane', '√©tanch√©it√©', 'rev√™tement', 'enduit',
            'carrelage', 'parquet', 'moquette', 'peinture', 'cr√©pi',
            'charpente', 'poutre', 'poteau', 'dalle', 'cloison'
        ]
        for material in materials_extended:
            if material in content.lower():
                entities['materials'].append(material)

        # Acteurs immobiliers
        real_estate_patterns = [
            r'ma√Ætre\s+d\'ouvrage',
            r'ma√Ætre\s+d\'≈ìuvre',
            r'architecte',
            r'entrepreneur',
            r'sous-traitant',
            r'bureau\s+d\'√©tudes',
            r'contr√¥leur\s+technique',
            r'coordonnateur\s+sps',
            r'g√©om√®tre',
            r'notaire',
            r'promoteur',
            r'lotisseur'
        ]
        for pattern in real_estate_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['real_estate_actors'].extend(matches)

        # Termes d'assurance
        insurance_patterns = [
            r'dommages?[-\s]ouvrage',
            r'd√©cennale',
            r'biennale',
            r'parfait\s+ach√®vement',
            r'responsabilit√©\s+civile',
            r'tous\s+risques?\s+chantier',
            r'garantie\s+financi√®re',
            r'caution\s+de\s+soumission'
        ]
        for pattern in insurance_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['insurance_terms'].extend(matches)

        # D√©lais sp√©cifiques
        deadline_patterns = [
            r'\d+\s*jours?\s*(?:ouvrables?|ouvr√©s?|calendaires?)?',
            r'\d+\s*semaines?',
            r'\d+\s*mois',
            r'\d+\s*ann√©es?',
            r'd√©lai\s+de\s+\d+',
            r'dans\s+les\s+\d+\s*(?:jours?|mois)',
            r'avant\s+le\s+\d+',
            r'√†\s+compter\s+du?'
        ]
        for pattern in deadline_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['deadlines'].extend(matches)

        # P√©nalit√©s et sanctions
        penalty_patterns = [
            r'p√©nalit√©s?\s*(?:de\s+retard)?',
            r'astreintes?',
            r'dommages?[-\s]int√©r√™ts',
            r'indemnit√©s?',
            r'r√©solution\s+du\s+contrat',
            r'r√©siliation',
            r'exclusion',
            r'\d+[\.,]?\d*\s*%\s*par\s*(?:jour|mois)\s*de\s*retard'
        ]
        for pattern in penalty_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['penalties'].extend(matches)

        # Sp√©cifications techniques
        tech_spec_patterns = [
            r'classe\s+[A-F]\d*',
            r'r√©sistance\s*:?\s*\d+[\.,]?\d*\s*MPa',
            r'pH\s*:?\s*\d+[\.,]?\d*',
            r'temp√©rature\s*:?\s*[\-\+]?\d+[\.,]?\d*\s*¬∞C',
            r'pression\s*:?\s*\d+[\.,]?\d*\s*bars?',
            r'viscosit√©\s*:?\s*\d+',
            r'granulom√©trie\s*:?\s*\d+[\.,]?\d*'
        ]
        for pattern in tech_spec_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['technical_specs'].extend(matches)

        return entities

    def classify_content(self, content: str) -> str:
        """Classification avanc√©e du type de contenu."""
        content_lower = content.lower()

        # Cat√©gories enrichies avec pond√©ration
        weighted_categories = {
            'financial': {
                'keywords': ['prix', 'co√ªt', 'tarif', 'montant', 'euros', '‚Ç¨', 'facture', 'paiement', 'acompte', 'devis', 'budget', 'tva', 'ht', 'ttc'],
                'patterns': [r'\d+[\s,]*\d*\s*(?:euros?|‚Ç¨)', r'\d+[\s,]*\d*\s*%', r'tva\s*:?\s*\d+'],
                'weight': 1.2
            },
            'timeline': {
                'keywords': ['d√©lai', 'livraison', '√©ch√©ance', 'date', 'planning', 'dur√©e', 'terme', 'calendrier', 'programmation'],
                'patterns': [r'\d+\s*(?:jours?|mois|semaines?)', r'avant\s+le', r'√†\s+compter\s+du?'],
                'weight': 1.1
            },
            'obligations': {
                'keywords': ['obligation', 'engage', 'doit', 'responsabilit√©', 'devoir', 'tenu', 'charge', 'incombe'],
                'patterns': [r'(?:doit|devra|s\'engage)\s+√†?', r'est\s+tenu\s+de'],
                'weight': 1.3
            },
            'guarantees': {
                'keywords': ['garantie', 'assurance', 'caution', 'couverture', 'protection', 'd√©cennale', 'dommages-ouvrage'],
                'patterns': [r'garantie\s+de', r'assurance\s+\w+', r'caution\s+de'],
                'weight': 1.2
            },
            'technical_requirements': {
                'keywords': ['technique', 'norme', 'sp√©cification', 'mat√©riau', 'DTU', 'performance', 'qualit√©', 'conformit√©', 'r√©sistance'],
                'patterns': [r'DTU\s+[\d\.]+', r'NF\s+EN', r'conforme\s+√†'],
                'weight': 1.1
            },
            'conditions': {
                'keywords': ['condition', 'clause', 'modalit√©', 'stipulation', 'disposition', 'si', 'sous\sr√©serve', 'sauf'],
                'patterns': [r'sous\s+(?:condition|r√©serve)', r'si\s+\w+', r'sauf\s+\w+'],
                'weight': 1.0
            },
            'quality_control': {
                'keywords': ['contr√¥le', 'v√©rification', 'test', 'essai', 'conformit√©', 'inspection', 'validation'],
                'patterns': [r'contr√¥le\s+de', r'v√©rification\s+de', r'conforme\s+aux?'],
                'weight': 1.0
            },
            'safety_security': {
                'keywords': ['s√©curit√©', 'protection', 'risque', 'danger', 'pr√©vention', 'accident', 'sps', 'epi'],
                'patterns': [r'plan\s+de\s+pr√©vention', r'mesures?\s+de\s+s√©curit√©'],
                'weight': 1.0
            },
            'administrative': {
                'keywords': ['autorisation', 'permis', 'd√©claration', 'formalit√©', 'proc√©dure', 'dossier'],
                'patterns': [r'permis\s+de', r'autorisation\s+de', r'd√©claration\s+de'],
                'weight': 0.9
            },
            'insurance': {
                'keywords': ['assurance', 'assureur', 'sinistre', 'indemnisation', 'prime', 'franchise'],
                'patterns': [r'police\s+d\'assurance', r'contrat\s+d\'assurance'],
                'weight': 1.1
            },
            'penalties': {
                'keywords': ['p√©nalit√©', 'amende', 'sanction', 'dommages-int√©r√™ts', 'astreinte', 'indemnit√©'],
                'patterns': [r'p√©nalit√©s?\s+de', r'\d+[\.,]?\d*\s*%\s*par'],
                'weight': 1.2
            }
        }

        scores = {}
        for category, data in weighted_categories.items():
            # Score des mots-cl√©s
            keyword_score = sum(1 for keyword in data['keywords'] if keyword in content_lower)

            # Score des patterns
            pattern_score = sum(len(re.findall(pattern, content_lower)) for pattern in data.get('patterns', []))

            # Score pond√©r√© final
            total_score = (keyword_score + pattern_score * 1.5) * data['weight']

            if total_score > 0:
                scores[category] = total_score

        if scores:
            # Seuil minimum pour √©viter les fausses classifications
            max_score = max(scores.values())
            if max_score >= 1.5:  # Seuil de confiance
                return max(scores, key=scores.get)

        return 'general'

    def get_classification_scores(self, content: str) -> Dict:
        """Obtenir les scores de classification pour tous les types."""
        content_lower = content.lower()

        categories = {
            'obligations': ['obligation', 'engage', 'doit', 'responsabilit√©'],
            'conditions': ['condition', 'clause', 'modalit√©', 'si'],
            'financial': ['prix', 'co√ªt', 'euros', '‚Ç¨', 'montant'],
            'timeline': ['d√©lai', 'date', '√©ch√©ance', 'livraison'],
            'guarantees': ['garantie', 'assurance', 'caution'],
            'technical_requirements': ['technique', 'norme', 'sp√©cification'],
            'quality_control': ['contr√¥le', 'v√©rification', 'test'],
            'safety_security': ['s√©curit√©', 'protection', 'risque'],
            'administrative': ['autorisation', 'permis', 'd√©claration'],
            'definitions': ['d√©finition', 'signifie', 'd√©signe'],
            'procedures': ['proc√©dure', 'm√©thode', '√©tape']
        }

        scores = {}
        for category, keywords in categories.items():
            scores[category] = sum(1 for keyword in keywords if keyword in content_lower)

        return scores

    def calculate_coherence(self, content: str) -> float:
        """Calculer la coh√©rence du contenu."""
        sentences = re.split(r'[.!?]+', content)
        if len(sentences) < 2:
            return 0.8

        # V√©rifier la pr√©sence de connecteurs
        connectors = ['et', 'ou', 'mais', 'donc', 'car', 'ainsi', 'alors', 'cependant', 'toutefois']
        connector_count = sum(1 for conn in connectors if conn in content.lower())

        coherence = min(1.0, 0.6 + (connector_count / len(sentences)) * 0.4)
        return round(coherence, 3)

    def calculate_relevance(self, content: str) -> float:
        """Calculer la pertinence juridique."""
        legal_terms = [
            'contrat', 'accord', 'convention', 'engagement', 'obligation',
            'droit', 'devoir', 'clause', 'article', 'disposition'
        ]

        words = content.lower().split()
        legal_word_count = sum(1 for word in words if word in legal_terms)

        relevance = min(1.0, 0.7 + (legal_word_count / len(words)) * 3)
        return round(relevance, 3)

    def calculate_factual_density(self, entities: Dict) -> float:
        """Calculer la densit√© factuelle bas√©e sur les entit√©s."""
        total_entities = sum(len(entity_list) for entity_list in entities.values())
        # Normaliser par rapport √† un chunk moyen
        density = min(1.0, total_entities / 5)
        return round(density, 3)

    def get_title(self, content: str) -> str:
        """Extraire ou g√©n√©rer un titre pour le chunk."""
        # Chercher un article num√©rot√©
        article_match = re.search(r'article\s+\d+[^\n.]*', content, re.IGNORECASE)
        if article_match:
            return article_match.group(0).strip()

        # Chercher une clause
        clause_match = re.search(r'clause\s+[^\n.]*', content, re.IGNORECASE)
        if clause_match:
            return clause_match.group(0).strip()

        # Premi√®re phrase courte
        sentences = re.split(r'[.!?]+', content)
        if sentences and len(sentences[0]) < 80:
            return sentences[0].strip()

        return "Clause contractuelle"

    def extract_key_elements(self, content: str) -> list:
        """Extraire les √©l√©ments cl√©s du chunk."""
        elements = []

        # Mots-cl√©s importants avec priorit√©
        priority_words = {
            'contrat': 3, 'prix': 3, 'd√©lai': 3, 'garantie': 3,
            'obligation': 2, 'clause': 2, 'article': 2, 'conditions': 2,
            'paiement': 2, 'livraison': 2, 'responsabilit√©': 2,
            'assurance': 1, 'modalit√©': 1, '√©ch√©ance': 1
        }

        content_lower = content.lower()
        word_scores = []

        for word, priority in priority_words.items():
            if word in content_lower:
                word_scores.append((word, priority))

        # Trier par priorit√© et prendre les 5 premiers
        word_scores.sort(key=lambda x: x[1], reverse=True)
        elements = [word for word, _ in word_scores[:5]]

        return elements

    def detect_document_type(self, text: str) -> str:
        """D√©tection du type de document."""
        text_lower = text.lower()

        patterns = {
            'contrat_reservation_vefa': ['vefa', 'r√©servation', 'futur ach√®vement'],
            'cctp': ['cctp', 'clauses techniques particuli√®res', 'cahier des charges'],
            'acte_notarie': ['acte', 'notaire', 'notari√©', '√©tude'],
            'bail_habitation': ['bail', 'location', 'logement', 'loyer'],
            'bail_commercial': ['bail commercial', 'fonds de commerce'],
            'marche_public': ['march√© public', 'appel d\'offres', 'soumission'],
            'permis_construire': ['permis de construire', 'autorisation', 'urbanisme'],
            'devis': ['devis', 'estimation', 'chiffrage'],
            'facture': ['facture', 'facturation', 'tva']
        }

        for doc_type, keywords in patterns.items():
            if any(keyword in text_lower for keyword in keywords):
                return doc_type

        return 'contrat_general'

# Instance du service avec initialisation
chunking_service = ChunkingService()

@app.get("/")
async def root():
    """Point d'entr√©e principal de l'API."""
    return {
        "message": "Legal Document Chunking API",
        "version": "1.0.0",
        "endpoints": {
            "chunk": "/chunk - POST - Chunking de documents",
            "health": "/health - GET - Status de l'API"
        }
    }

@app.get("/health")
async def health():
    """Endpoint de sant√© pour v√©rification."""
    return {
        "status": "healthy",
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

@app.post("/chunk")
async def chunk_document(request: ChunkingRequest):
    """Endpoint principal pour le chunking de documents."""
    try:
        extracted_text = request.extractedText.strip()

        if not extracted_text:
            raise HTTPException(
                status_code=400,
                detail="Le champ extractedText est requis et ne peut pas √™tre vide"
            )

        if len(extracted_text) < 100:
            raise HTTPException(
                status_code=400,
                detail="Le texte doit contenir au moins 100 caract√®res"
            )

        # Options avec valeurs par d√©faut
        options = request.options
        target_chunk_size = options.get('target_chunk_size', 60)
        overlap_size = options.get('overlap_size', 15)

        # Validation des param√®tres
        if not (20 <= target_chunk_size <= 200):
            raise HTTPException(
                status_code=400,
                detail="target_chunk_size doit √™tre entre 20 et 200 mots"
            )

        if not (0 <= overlap_size <= 50):
            raise HTTPException(
                status_code=400,
                detail="overlap_size doit √™tre entre 0 et 50 mots"
            )

        # Traitement du chunking
        start_time = time.time()

        chunks = chunking_service.create_smart_chunks(
            extracted_text,
            target_chunk_size,
            overlap_size
        )

        end_time = time.time()
        processing_time = int((end_time - start_time) * 1000)

        # Calcul des statistiques
        quality_dist = {'high': 0, 'medium': 0, 'low': 0}
        total_quality = 0

        for chunk in chunks:
            score = chunk['quality_analysis']['overall_score']
            total_quality += score

            if score >= 0.8:
                quality_dist['high'] += 1
            elif score >= 0.5:
                quality_dist['medium'] += 1
            else:
                quality_dist['low'] += 1

        avg_quality = total_quality / len(chunks) if chunks else 0

        # Validation des r√©sultats
        low_quality_rate = (quality_dist['low'] / len(chunks) * 100) if chunks else 0

        validation_results = {
            'consistency_score': 0.85,
            'cross_reference_integrity': 0.90,
            'missing_elements': [],
            'recommendations': []
        }

        if low_quality_rate > 30:
            validation_results['recommendations'].append(
                "Taux √©lev√© de chunks de basse qualit√© - consid√©rer l'augmentation de target_chunk_size"
            )

        # R√©ponse finale
        response_data = {
            'success': True,
            'chunks': chunks,
            'document_stats': {
                'document_type': chunking_service.detect_document_type(extracted_text),
                'estimated_complexity': 'medium',
                'processing_time_ms': processing_time,
                'total_chunks': len(chunks),
                'avg_chunk_quality': round(avg_quality, 3),
                'quality_distribution': quality_dist,
                'text_length': len(extracted_text),
                'avg_chunk_size': sum(chunk['metadata']['word_count'] for chunk in chunks) / len(chunks) if chunks else 0
            },
            'validation_results': validation_results,
            'processing_info': {
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'version': '1.0.0',
                'target_chunk_size': target_chunk_size,
                'overlap_size': overlap_size
            }
        }

        return JSONResponse(content=response_data)

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Erreur interne du serveur: {str(e)}"
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)