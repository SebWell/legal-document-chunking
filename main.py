"""
FastAPI server for Legal Document Chunking
Designed for VPS deployment with n8n
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Optional, Dict, Any
import json
import re
import time
from datetime import datetime, timezone

app = FastAPI(
    title="Legal Document Chunking API",
    description="API pour le chunking intelligent de documents juridiques franÃ§ais",
    version="2.0.0"
)

# Force OpenAPI 3.0.2 pour compatibilitÃ© avec Swagger UI
def custom_openapi():
    if app.openapi_schema:
        return app.openapi_schema
    from fastapi.openapi.utils import get_openapi
    openapi_schema = get_openapi(
        title=app.title,
        version=app.version,
        description=app.description,
        routes=app.routes,
        openapi_version="3.0.2"
    )
    app.openapi_schema = openapi_schema
    return app.openapi_schema

app.openapi = custom_openapi

# CORS pour n8n
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChunkingRequest(BaseModel):
    extractedText: str
    options: Optional[Dict[str, Any]] = {}

class ChunkingService:
    """Service de chunking intelligent pour documents juridiques - Version Pro."""

    def __init__(self):
        # Dictionnaires spÃ©cialisÃ©s pour l'immobilier
        self.real_estate_terms = {
            'construction': ['gros Å“uvre', 'second Å“uvre', 'fondations', 'charpente', 'couverture', 'Ã©tanchÃ©itÃ©', 'isolation', 'cloisons', 'revÃªtements'],
            'legal_actors': ['maÃ®tre d\'ouvrage', 'maÃ®tre d\'Å“uvre', 'architecte', 'entrepreneur', 'sous-traitant', 'bureau d\'Ã©tudes', 'contrÃ´leur technique'],
            'insurance': ['dommages-ouvrage', 'dÃ©cennale', 'biennale', 'parfait achÃ¨vement', 'responsabilitÃ© civile', 'tous risques chantier'],
            'certifications': ['RT2012', 'RT2020', 'HQE', 'BBC', 'BEPOS', 'NF Habitat', 'Qualitel'],
            'procedures': ['permis de construire', 'dÃ©claration prÃ©alable', 'DT-DICT', 'rÃ©ception des travaux', 'levÃ©e des rÃ©serves'],
            'financial_terms': ['rÃ©vision de prix', 'actualisation', 'variation', 'acompte', 'avancement', 'retenue de garantie', 'dÃ©compte']
        }

    def create_smart_chunks(self, text: str, target_size: int = 60, overlap: int = 15):
        """CrÃ©er des chunks intelligents avec gestion avancÃ©e des structures."""
        # Nettoyage et prÃ©paration du texte
        text = self.preprocess_text(text)

        # DÃ©tection des structures spÃ©ciales
        special_sections = self.detect_special_structures(text)

        # Adaptation de la taille selon le contenu
        adaptive_size = self.calculate_adaptive_size(text, target_size)

        # Traitement selon la structure dÃ©tectÃ©e
        if special_sections['has_tables']:
            return self.chunk_with_table_handling(text, adaptive_size, overlap)
        else:
            return self.chunk_standard_content(text, adaptive_size, overlap)

    def preprocess_text(self, text: str) -> str:
        """PrÃ©traitement avancÃ© du texte."""
        # Nettoyage des espaces multiples
        text = re.sub(r'\s+', ' ', text.strip())

        # Normalisation des caractÃ¨res spÃ©ciaux
        text = re.sub(r'[\u00A0\u2000-\u200B\u2028\u2029]', ' ', text)

        # Protection des structures importantes
        text = self.protect_legal_structures(text)

        return text

    def protect_legal_structures(self, text: str) -> str:
        """ProtÃ©ger les structures juridiques importantes."""
        # ProtÃ©ger les rÃ©fÃ©rences d'articles
        text = re.sub(r'(article\s+[a-z]?\d+[\-\d]*)', r'\1ğŸ”’', text, flags=re.IGNORECASE)

        # ProtÃ©ger les montants
        text = re.sub(r'(\d+[\s,]*\d*\s*(?:euros?|â‚¬))', r'\1ğŸ”’', text, flags=re.IGNORECASE)

        return text

    def detect_special_structures(self, text: str) -> dict:
        """DÃ©tecter les structures spÃ©ciales dans le document."""
        return {
            'has_tables': '|' in text or 'Nom / DÃ©nomination' in text,
            'has_numbered_clauses': bool(re.search(r'^\s*\d+[\./)]', text, re.MULTILINE)),
            'has_legal_formulas': bool(re.search(r'\$[^\$]+\$', text)),
            'has_complex_references': len(re.findall(r'article\s+[a-z]?\d+', text, re.IGNORECASE)) > 5
        }

    def calculate_adaptive_size(self, text: str, base_size: int) -> int:
        """Calculer la taille adaptative selon le type de contenu."""
        content_type = self.classify_global_content(text)

        size_adjustments = {
            'financial': base_size + 10,  # Plus de contexte pour les clauses financiÃ¨res
            'technical_requirements': base_size + 15,  # SpÃ©cifications techniques dÃ©taillÃ©es
            'obligations': base_size - 5,  # Obligations plus concises
            'timeline': base_size,
            'legal_references': base_size - 10  # Articles courts et prÃ©cis
        }

        return size_adjustments.get(content_type, base_size)

    def classify_global_content(self, text: str) -> str:
        """Classification globale du document."""
        text_lower = text.lower()

        # Calcul des scores pour chaque catÃ©gorie
        category_scores = {
            'financial': len(re.findall(r'prix|coÃ»t|euros?|â‚¬|montant|facture|paiement|devis', text_lower)),
            'technical_requirements': len(re.findall(r'technique|norme|spÃ©cification|matÃ©riau|dtu|performance|qualitÃ©', text_lower)),
            'obligations': len(re.findall(r'obligation|engage|doit|responsabilitÃ©|devoir|tenu', text_lower)),
            'timeline': len(re.findall(r'dÃ©lai|livraison|Ã©chÃ©ance|date|planning|durÃ©e', text_lower)),
            'legal_references': len(re.findall(r'article|dÃ©cret|loi|code|rÃ©fÃ©rence', text_lower))
        }

        return max(category_scores, key=category_scores.get) if any(category_scores.values()) else 'general'

    def chunk_with_table_handling(self, text: str, target_size: int, overlap: int):
        """Chunking spÃ©cialisÃ© pour les documents avec tableaux."""
        chunks = []
        chunk_id = 1

        # SÃ©paration du contenu par sections
        sections = self.split_by_tables(text)

        for section in sections:
            if self.is_table_content(section):
                # Traitement spÃ©cialisÃ© pour les tableaux
                table_chunks = self.chunk_table_content(section, chunk_id)
                chunks.extend(table_chunks)
                chunk_id += len(table_chunks)
            else:
                # Traitement standard pour le texte normal
                section_chunks = self.chunk_standard_content(section, target_size, overlap, start_id=chunk_id)
                chunks.extend(section_chunks)
                chunk_id += len(section_chunks)

        return chunks

    def split_by_tables(self, text: str) -> list:
        """SÃ©parer le texte en sections normales et tableaux."""
        # Pattern pour dÃ©tecter les dÃ©buts de tableaux
        table_pattern = r'(\|[^|]*\|[^|]*\|)'

        sections = []
        last_end = 0

        for match in re.finditer(table_pattern, text):
            # Ajouter le texte avant le tableau
            if match.start() > last_end:
                sections.append(text[last_end:match.start()])

            # Trouver la fin du tableau
            table_start = match.start()
            table_end = self.find_table_end(text, table_start)
            sections.append(text[table_start:table_end])
            last_end = table_end

        # Ajouter le texte restant
        if last_end < len(text):
            sections.append(text[last_end:])

        return [s.strip() for s in sections if s.strip()]

    def find_table_end(self, text: str, start: int) -> int:
        """Trouver la fin d'un tableau."""
        lines = text[start:].split('\n')
        end_line = 0

        for i, line in enumerate(lines):
            if '|' not in line and line.strip() and not re.match(r'^\s*[-:]+\s*$', line):
                break
            end_line = i + 1

        return start + len('\n'.join(lines[:end_line]))

    def is_table_content(self, section: str) -> bool:
        """VÃ©rifier si une section contient un tableau."""
        return '|' in section and section.count('|') >= 4

    def chunk_table_content(self, table_text: str, start_id: int) -> list:
        """Chunking spÃ©cialisÃ© pour les tableaux."""
        chunks = []
        lines = table_text.strip().split('\n')

        # Grouper les lignes de tableau en chunks logiques
        current_group = []
        header_processed = False

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # Conserver l'en-tÃªte avec chaque groupe
            if not header_processed and '|' in line and not re.match(r'^\s*[-:]+\s*$', line):
                header = line
                header_processed = True
                current_group.append(line)
            elif re.match(r'^\s*[-:]+\s*$', line):
                current_group.append(line)
            else:
                current_group.append(line)

                # CrÃ©er un chunk tous les 3-4 lignes de donnÃ©es
                if len(current_group) >= 4:
                    chunk_content = '\n'.join(current_group)
                    chunk = self.create_chunk(chunk_content, start_id)
                    chunks.append(chunk)

                    # RedÃ©marrer avec l'en-tÃªte
                    current_group = [header] if 'header' in locals() else []
                    start_id += 1

        # Chunk final s'il reste des donnÃ©es
        if current_group and len(current_group) > 1:
            chunk_content = '\n'.join(current_group)
            chunk = self.create_chunk(chunk_content, start_id)
            chunks.append(chunk)

        return chunks

    def chunk_standard_content(self, text: str, target_size: int, overlap: int, start_id: int = 1):
        """Chunking standard amÃ©liorÃ©."""
        # DÃ©coupage par phrases avec gestion des abrÃ©viations
        sentences = self.smart_sentence_split(text)
        chunks = []
        current_chunk = []
        current_word_count = 0
        chunk_id = start_id

        for sentence in sentences:
            if not sentence.strip():
                continue

            sentence_words = sentence.split()

            # VÃ©rification de la cohÃ©sion avant de couper
            if (current_word_count + len(sentence_words) > target_size and
                current_chunk and self.can_split_here(sentence, current_chunk)):

                # CrÃ©er le chunk actuel
                chunk_content = ' '.join(current_chunk)
                chunk = self.create_chunk(chunk_content, chunk_id)
                chunks.append(chunk)

                # PrÃ©parer le chunk suivant avec overlap sÃ©mantique
                overlap_words = self.get_semantic_overlap(current_chunk, overlap)
                current_chunk = overlap_words + sentence_words
                current_word_count = len(current_chunk)
                chunk_id += 1
            else:
                # Ajouter la phrase au chunk actuel
                current_chunk.extend(sentence_words)
                current_word_count += len(sentence_words)

        # Chunk final
        if current_chunk:
            chunk_content = ' '.join(current_chunk)
            chunk = self.create_chunk(chunk_content, chunk_id)
            chunks.append(chunk)

        return chunks

    def smart_sentence_split(self, text: str) -> list:
        """DÃ©coupage intelligent des phrases avec gestion des abrÃ©viations."""
        # ProtÃ©ger les abrÃ©viations courantes
        abbreviations = ['art.', 'etc.', 'cf.', 'p.ex.', 'c.-Ã -d.', 'M.', 'Mme', 'Dr.', 'Me']

        protected_text = text
        for abbr in abbreviations:
            protected_text = protected_text.replace(abbr, abbr.replace('.', 'ğŸ”’'))

        # DÃ©coupage par phrases
        sentences = re.split(r'(?<=[.!?])\s+', protected_text)

        # Restaurer les points
        sentences = [s.replace('ğŸ”’', '.') for s in sentences]

        return sentences

    def can_split_here(self, next_sentence: str, current_chunk: list) -> bool:
        """VÃ©rifier si on peut couper Ã  cet endroit sans nuire Ã  la cohÃ©sion."""
        if not current_chunk:
            return True

        # Ne pas couper si la phrase suivante commence par une conjonction
        conjunctions = ['et', 'ou', 'mais', 'donc', 'car', 'ainsi', 'alors', 'cependant', 'toutefois', 'nÃ©anmoins']
        first_word = next_sentence.strip().split()[0].lower() if next_sentence.strip() else ''

        if first_word in conjunctions:
            return False

        # Ne pas couper au milieu d'une Ã©numÃ©ration
        last_chunk_text = ' '.join(current_chunk[-10:])  # Derniers mots
        if re.search(r'[;:]\s*$', last_chunk_text):
            return False

        return True

    def get_semantic_overlap(self, current_chunk: list, overlap_size: int) -> list:
        """Obtenir un overlap sÃ©mantique plutÃ´t que simplement les derniers mots."""
        if overlap_size == 0 or not current_chunk:
            return []

        # Chercher la derniÃ¨re phrase complÃ¨te dans la limite de l'overlap
        chunk_text = ' '.join(current_chunk)
        sentences = re.split(r'(?<=[.!?])\s+', chunk_text)

        # Prendre la derniÃ¨re phrase si elle fait moins que l'overlap
        if sentences and len(sentences[-1].split()) <= overlap_size:
            return sentences[-1].split()

        # Sinon, prendre les derniers mots
        return current_chunk[-overlap_size:] if len(current_chunk) >= overlap_size else current_chunk

    def create_chunk(self, content: str, chunk_id: int):
        """CrÃ©er un chunk avec mÃ©tadonnÃ©es et analyse de qualitÃ©."""
        word_count = len(content.split())

        # Analyse de qualitÃ©
        quality_score = self.analyze_quality(content)

        # Extraction d'entitÃ©s
        entities = self.extract_entities(content)

        # Classification du contenu
        content_type = self.classify_content(content)

        return {
            'id': f'chunk_{chunk_id:03d}',
            'content': content,
            'hierarchical_title': self.get_title(content),
            'content_type': content_type,
            'section_info': {
                'type': 'content',
                'number': chunk_id,
                'title': self.get_title(content)
            },
            'content_classification': {
                'type': content_type,
                'confidence': 0.85,
                'alternatives': [],
                'scores': self.get_classification_scores(content)
            },
            'metadata': {
                'word_count': word_count,
                'char_count': len(content),
                'position': chunk_id,
                'has_legal_references': bool(entities['legal_references']),
                'has_financial_info': bool(entities['monetary_amounts']),
                'has_dates': bool(entities['dates'])
            },
            'quality_analysis': {
                'overall_score': quality_score,
                'completeness': min(1.0, word_count / 50),
                'coherence': self.calculate_coherence(content),
                'relevance': self.calculate_relevance(content),
                'factual_density': self.calculate_factual_density(entities)
            },
            'extracted_entities': entities,
            'key_elements': self.extract_key_elements(content)
        }

    def analyze_quality(self, content: str) -> float:
        """Analyse de qualitÃ© avancÃ©e basÃ©e sur de multiples facteurs."""
        words = content.split()
        word_count = len(words)

        # 1. Facteur longueur optimisÃ© (courbe gaussienne)
        optimal_length = 55  # Taille optimale pour l'immobilier
        length_variance = 25
        length_factor = max(0.3, 1.0 - ((word_count - optimal_length) ** 2) / (2 * length_variance ** 2))

        # 2. Facteur mots-clÃ©s immobiliers pondÃ©rÃ©s
        weighted_keywords = {
            # Mots-clÃ©s prioritaires (poids 3)
            'contrat': 3, 'prix': 3, 'dÃ©lai': 3, 'garantie': 3, 'obligation': 3,
            # Mots-clÃ©s importants (poids 2)
            'article': 2, 'clause': 2, 'conditions': 2, 'responsabilitÃ©': 2,
            'livraison': 2, 'paiement': 2, 'travaux': 2, 'entreprise': 2,
            # Mots-clÃ©s secondaires (poids 1)
            'partie': 1, 'engagement': 1, 'modalitÃ©': 1, 'Ã©chÃ©ance': 1,
            'conformitÃ©': 1, 'exÃ©cution': 1, 'rÃ©ception': 1, 'achevements': 1
        }

        keyword_score = sum(weighted_keywords.get(word.lower(), 0) for word in words)
        max_possible_score = word_count * 3  # Si tous les mots Ã©taient prioritaires
        keyword_factor = min(1.0, keyword_score / max(max_possible_score * 0.2, 1))

        # 3. Facteur entitÃ©s enrichi
        entities = self.extract_entities(content)
        entity_score = 0

        # Dates (important pour les dÃ©lais)
        if entities['dates']:
            entity_score += 0.15
        if entities['deadlines']:
            entity_score += 0.1

        # Montants financiers (critiques)
        if entities['monetary_amounts']:
            entity_score += 0.2

        # RÃ©fÃ©rences lÃ©gales (fond juridique)
        if entities['legal_references']:
            entity_score += 0.15

        # Mesures et spÃ©cifications (technique)
        if entities['measurements'] or entities['technical_specs']:
            entity_score += 0.1

        # Acteurs et assurances (contextuel)
        if entities['real_estate_actors'] or entities['insurance_terms']:
            entity_score += 0.1

        # Normes (qualitÃ© technique)
        if entities['norms_standards']:
            entity_score += 0.1

        entity_factor = min(1.0, 0.4 + entity_score)  # Base 0.4 + bonus

        # 4. Facteur structure avancÃ©
        complete_sentences = len(re.findall(r'[.!?]', content))

        # Bonus pour la structure logique
        has_enumeration = bool(re.search(r'[;:]\s*(?:\n|$)', content))
        has_paragraphs = '\n' in content.strip()
        has_proper_punctuation = complete_sentences > 0

        structure_base = min(1.0, complete_sentences / 2)
        structure_bonus = 0.1 * has_enumeration + 0.1 * has_paragraphs + 0.1 * has_proper_punctuation
        structure_factor = min(1.0, structure_base + structure_bonus)

        # 5. Facteur cohÃ©sion sÃ©mantique
        coherence_factor = self.calculate_semantic_coherence(content, words)

        # 6. Facteur spÃ©cificitÃ© immobilier
        real_estate_terms = [
            'construction', 'bÃ¢timent', 'immeuble', 'logement', 'terrain',
            'permis', 'urbanisme', 'maÃ®tre d\'ouvrage', 'architecte',
            'entrepreneur', 'chantier', 'rÃ©ception', 'livraison', 'conformitÃ©'
        ]
        re_term_count = sum(1 for term in real_estate_terms if term in content.lower())
        specificity_factor = min(1.0, 0.7 + (re_term_count / len(real_estate_terms)) * 0.3)

        # Score final pondÃ©rÃ© optimisÃ©
        score = (
            0.25 * length_factor +      # Longueur optimale
            0.20 * keyword_factor +     # Mots-clÃ©s juridiques
            0.25 * entity_factor +      # Richesse des entitÃ©s
            0.15 * structure_factor +   # Structure du texte
            0.10 * coherence_factor +   # CohÃ©sion sÃ©mantique
            0.05 * specificity_factor   # SpÃ©cificitÃ© immobilier
        )

        return round(min(1.0, score), 3)

    def calculate_semantic_coherence(self, content: str, words: list) -> float:
        """Calculer la cohÃ©sion sÃ©mantique du contenu."""
        if len(words) < 10:
            return 0.5

        # DÃ©tecter la rÃ©pÃ©tition excessive
        unique_words = set(word.lower() for word in words if len(word) > 3)
        repetition_ratio = len(unique_words) / len([w for w in words if len(w) > 3])
        repetition_factor = min(1.0, repetition_ratio * 2)  # PÃ©naliser la rÃ©pÃ©tition

        # DÃ©tecter les connecteurs logiques
        connectors = [
            'et', 'ou', 'mais', 'donc', 'car', 'ainsi', 'alors', 'cependant',
            'toutefois', 'nÃ©anmoins', 'par consÃ©quent', 'en effet', 'de plus',
            'en outre', 'notamment', 'c\'est-Ã -dire', 'autrement dit'
        ]
        connector_count = sum(1 for conn in connectors if conn in content.lower())
        connector_factor = min(1.0, 0.6 + (connector_count / len(content.split())) * 10)

        # DÃ©tecter la continuitÃ© thÃ©matique
        theme_consistency = self.calculate_theme_consistency(content)

        return round((repetition_factor + connector_factor + theme_consistency) / 3, 3)

    def calculate_theme_consistency(self, content: str) -> float:
        """Calculer la consistance thÃ©matique."""
        content_lower = content.lower()

        # DÃ©finir les thÃ¨mes principaux
        themes = {
            'contractuel': ['contrat', 'clause', 'obligation', 'engagement', 'accord'],
            'financier': ['prix', 'coÃ»t', 'montant', 'paiement', 'facture', 'devis'],
            'temporel': ['dÃ©lai', 'livraison', 'Ã©chÃ©ance', 'planning', 'durÃ©e'],
            'technique': ['travaux', 'construction', 'matÃ©riaux', 'norme', 'qualitÃ©'],
            'juridique': ['responsabilitÃ©', 'garantie', 'assurance', 'droit', 'loi']
        }

        theme_scores = {}
        for theme_name, keywords in themes.items():
            score = sum(1 for keyword in keywords if keyword in content_lower)
            if score > 0:
                theme_scores[theme_name] = score

        if not theme_scores:
            return 0.5

        # Calculer la dominance du thÃ¨me principal
        max_score = max(theme_scores.values())
        total_score = sum(theme_scores.values())

        # Un contenu cohÃ©rent a un thÃ¨me dominant mais pas exclusif
        theme_dominance = max_score / total_score if total_score > 0 else 0

        # Optimum autour de 0.4-0.6 (pas trop dispersÃ©, pas trop mono-thÃ©matique)
        if 0.3 <= theme_dominance <= 0.7:
            return min(1.0, 0.8 + (1 - abs(theme_dominance - 0.5)) * 0.4)
        else:
            return max(0.3, 0.8 - abs(theme_dominance - 0.5))

    def extract_entities(self, content: str) -> Dict:
        """Extraction d'entitÃ©s juridiques et techniques avancÃ©e."""
        entities = {
            'dates': [],
            'monetary_amounts': [],
            'legal_references': [],
            'measurements': [],
            'norms_standards': [],
            'materials': [],
            'technical_specs': [],
            'real_estate_actors': [],
            'insurance_terms': [],
            'deadlines': [],
            'penalties': []
        }

        # Dates (formats franÃ§ais Ã©tendus)
        date_patterns = [
            r'\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{2,4}',
            r'\d{1,2}\s+(?:janvier|fÃ©vrier|mars|avril|mai|juin|juillet|aoÃ»t|septembre|octobre|novembre|dÃ©cembre)\s+\d{4}',
            r'(?:janvier|fÃ©vrier|mars|avril|mai|juin|juillet|aoÃ»t|septembre|octobre|novembre|dÃ©cembre)\s+\d{4}',
            r'\d{1,2}er?\s+(?:janvier|fÃ©vrier|mars|avril|mai|juin|juillet|aoÃ»t|septembre|octobre|novembre|dÃ©cembre)\s+\d{4}',
            r'(?:lundi|mardi|mercredi|jeudi|vendredi|samedi|dimanche)\s+\d{1,2}'
        ]
        for pattern in date_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['dates'].extend(matches)

        # Montants et devises avancÃ©s
        amount_patterns = [
            r'\d+[\s,]*\d*[\.,]\d{2}\s*(?:euros?|â‚¬)',
            r'\d+[\s,]*\d*\s*(?:euros?|â‚¬)',
            r'\d+[\s,]*\d*\s*EUR',
            r'\d+[\s,]*\d*\s*(?:\$|dollars?)',
            r'(?:prix|coÃ»t|montant|tarif)\s*:?\s*\d+[\s,]*\d*',
            r'\d+[\s,]*\d*\s*(?:kâ‚¬|Kâ‚¬|milliers?\s*d\'euros?)',
            r'TVA\s*:?\s*\d+[\.,]?\d*\s*%?',
            r'HT|TTC'
        ]
        for pattern in amount_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['monetary_amounts'].extend(matches)

        # RÃ©fÃ©rences lÃ©gales Ã©tendues
        legal_patterns = [
            r'article\s+[a-z]?\d+[\-\d]*(?:\s*bis|ter|quater)?',
            r'[Ll]\s*\d+[\-\d]*',
            r'[Rr]\s*\d+[\-\d]*',
            r'dÃ©cret\s+nÂ°?\s*[\d\-]+',
            r'loi\s+nÂ°?\s*[\d\-]+',
            r'code\s+(?:civil|pÃ©nal|de\s+(?:la\s+)?construction|du\s+travail|de\s+l\'urbanisme)',
            r'CCH|CGCT|CPC|CPP',
            r'arrÃªtÃ©\s+(?:du\s+)?\d+',
            r'circulaire\s+nÂ°?\s*[\d\-]+'
        ]
        for pattern in legal_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['legal_references'].extend(matches)

        # Mesures et spÃ©cifications techniques Ã©tendues
        measure_patterns = [
            r'\d+[\.,]?\d*\s*(?:mÂ²|m2|mÃ¨tres?\s*carrÃ©s?)',
            r'\d+[\.,]?\d*\s*(?:mÂ³|m3|mÃ¨tres?\s*cubes?)',
            r'\d+[\.,]?\d*\s*(?:ml?|mÃ¨tres?\s*linÃ©aires?)',
            r'\d+[\.,]?\d*\s*(?:cm|centimÃ¨tres?)',
            r'\d+[\.,]?\d*\s*(?:mm|millimÃ¨tres?)',
            r'\d+[\.,]?\d*\s*(?:kg|kilogrammes?)',
            r'\d+[\.,]?\d*\s*(?:tonnes?)',
            r'\d+[\.,]?\d*\s*(?:%|pour\s*cent)',
            r'\d+[\.,]?\d*\s*(?:kW|MW|watts?)',
            r'\d+[\.,]?\d*\s*(?:Â°C|degrÃ©s?)',
            r'Ã©paisseur\s*:?\s*\d+[\.,]?\d*\s*(?:cm|mm)',
            r'hauteur\s*:?\s*\d+[\.,]?\d*\s*m'
        ]
        for pattern in measure_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['measurements'].extend(matches)

        # Normes et standards BTP Ã©tendus
        norm_patterns = [
            r'DTU\s+[\d\.]+',
            r'NF\s+EN\s+[\d\-]+',
            r'NF\s+[ABCP]\d+[\-\d]*',
            r'ISO\s+[\d\-]+',
            r'EN\s+[\d\-]+',
            r'AFNOR\s+[\w\d\-]+',
            r'RT\s*20(?:05|12|20)',
            r'RE\s*2020',
            r'BBC|HQE|BEPOS|HPE',
            r'Effinergie|Passivhaus',
            r'QUALIBAT|RGE|OPQIBI'
        ]
        for pattern in norm_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['norms_standards'].extend(matches)

        # MatÃ©riaux BTP Ã©tendus
        materials_extended = [
            'bÃ©ton', 'acier', 'bois', 'plÃ¢tre', 'ciment', 'sable', 'gravier',
            'parpaing', 'brique', 'tuile', 'ardoise', 'zinc', 'cuivre',
            'aluminium', 'PVC', 'polystyrÃ¨ne', 'laine de verre', 'laine de roche',
            'isolant', 'membrane', 'Ã©tanchÃ©itÃ©', 'revÃªtement', 'enduit',
            'carrelage', 'parquet', 'moquette', 'peinture', 'crÃ©pi',
            'charpente', 'poutre', 'poteau', 'dalle', 'cloison'
        ]
        for material in materials_extended:
            if material in content.lower():
                entities['materials'].append(material)

        # Acteurs immobiliers
        real_estate_patterns = [
            r'maÃ®tre\s+d\'ouvrage',
            r'maÃ®tre\s+d\'Å“uvre',
            r'architecte',
            r'entrepreneur',
            r'sous-traitant',
            r'bureau\s+d\'Ã©tudes',
            r'contrÃ´leur\s+technique',
            r'coordonnateur\s+sps',
            r'gÃ©omÃ¨tre',
            r'notaire',
            r'promoteur',
            r'lotisseur'
        ]
        for pattern in real_estate_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['real_estate_actors'].extend(matches)

        # Termes d'assurance
        insurance_patterns = [
            r'dommages?[-\s]ouvrage',
            r'dÃ©cennale',
            r'biennale',
            r'parfait\s+achÃ¨vement',
            r'responsabilitÃ©\s+civile',
            r'tous\s+risques?\s+chantier',
            r'garantie\s+financiÃ¨re',
            r'caution\s+de\s+soumission'
        ]
        for pattern in insurance_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['insurance_terms'].extend(matches)

        # DÃ©lais spÃ©cifiques
        deadline_patterns = [
            r'\d+\s*jours?\s*(?:ouvrables?|ouvrÃ©s?|calendaires?)?',
            r'\d+\s*semaines?',
            r'\d+\s*mois',
            r'\d+\s*annÃ©es?',
            r'dÃ©lai\s+de\s+\d+',
            r'dans\s+les\s+\d+\s*(?:jours?|mois)',
            r'avant\s+le\s+\d+',
            r'Ã \s+compter\s+du?'
        ]
        for pattern in deadline_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['deadlines'].extend(matches)

        # PÃ©nalitÃ©s et sanctions
        penalty_patterns = [
            r'pÃ©nalitÃ©s?\s*(?:de\s+retard)?',
            r'astreintes?',
            r'dommages?[-\s]intÃ©rÃªts',
            r'indemnitÃ©s?',
            r'rÃ©solution\s+du\s+contrat',
            r'rÃ©siliation',
            r'exclusion',
            r'\d+[\.,]?\d*\s*%\s*par\s*(?:jour|mois)\s*de\s*retard'
        ]
        for pattern in penalty_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['penalties'].extend(matches)

        # SpÃ©cifications techniques
        tech_spec_patterns = [
            r'classe\s+[A-F]\d*',
            r'rÃ©sistance\s*:?\s*\d+[\.,]?\d*\s*MPa',
            r'pH\s*:?\s*\d+[\.,]?\d*',
            r'tempÃ©rature\s*:?\s*[\-\+]?\d+[\.,]?\d*\s*Â°C',
            r'pression\s*:?\s*\d+[\.,]?\d*\s*bars?',
            r'viscositÃ©\s*:?\s*\d+',
            r'granulomÃ©trie\s*:?\s*\d+[\.,]?\d*'
        ]
        for pattern in tech_spec_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            entities['technical_specs'].extend(matches)

        return entities

    def classify_content(self, content: str) -> str:
        """Classification avancÃ©e du type de contenu."""
        content_lower = content.lower()

        # CatÃ©gories enrichies avec pondÃ©ration
        weighted_categories = {
            'financial': {
                'keywords': ['prix', 'coÃ»t', 'tarif', 'montant', 'euros', 'â‚¬', 'facture', 'paiement', 'acompte', 'devis', 'budget', 'tva', 'ht', 'ttc'],
                'patterns': [r'\d+[\s,]*\d*\s*(?:euros?|â‚¬)', r'\d+[\s,]*\d*\s*%', r'tva\s*:?\s*\d+'],
                'weight': 1.2
            },
            'timeline': {
                'keywords': ['dÃ©lai', 'livraison', 'Ã©chÃ©ance', 'date', 'planning', 'durÃ©e', 'terme', 'calendrier', 'programmation'],
                'patterns': [r'\d+\s*(?:jours?|mois|semaines?)', r'avant\s+le', r'Ã \s+compter\s+du?'],
                'weight': 1.1
            },
            'obligations': {
                'keywords': ['obligation', 'engage', 'doit', 'responsabilitÃ©', 'devoir', 'tenu', 'charge', 'incombe'],
                'patterns': [r'(?:doit|devra|s\'engage)\s+Ã ?', r'est\s+tenu\s+de'],
                'weight': 1.3
            },
            'guarantees': {
                'keywords': ['garantie', 'assurance', 'caution', 'couverture', 'protection', 'dÃ©cennale', 'dommages-ouvrage'],
                'patterns': [r'garantie\s+de', r'assurance\s+\w+', r'caution\s+de'],
                'weight': 1.2
            },
            'technical_requirements': {
                'keywords': ['technique', 'norme', 'spÃ©cification', 'matÃ©riau', 'DTU', 'performance', 'qualitÃ©', 'conformitÃ©', 'rÃ©sistance'],
                'patterns': [r'DTU\s+[\d\.]+', r'NF\s+EN', r'conforme\s+Ã '],
                'weight': 1.1
            },
            'conditions': {
                'keywords': ['condition', 'clause', 'modalitÃ©', 'stipulation', 'disposition', 'si', 'sous\srÃ©serve', 'sauf'],
                'patterns': [r'sous\s+(?:condition|rÃ©serve)', r'si\s+\w+', r'sauf\s+\w+'],
                'weight': 1.0
            },
            'quality_control': {
                'keywords': ['contrÃ´le', 'vÃ©rification', 'test', 'essai', 'conformitÃ©', 'inspection', 'validation'],
                'patterns': [r'contrÃ´le\s+de', r'vÃ©rification\s+de', r'conforme\s+aux?'],
                'weight': 1.0
            },
            'safety_security': {
                'keywords': ['sÃ©curitÃ©', 'protection', 'risque', 'danger', 'prÃ©vention', 'accident', 'sps', 'epi'],
                'patterns': [r'plan\s+de\s+prÃ©vention', r'mesures?\s+de\s+sÃ©curitÃ©'],
                'weight': 1.0
            },
            'administrative': {
                'keywords': ['autorisation', 'permis', 'dÃ©claration', 'formalitÃ©', 'procÃ©dure', 'dossier'],
                'patterns': [r'permis\s+de', r'autorisation\s+de', r'dÃ©claration\s+de'],
                'weight': 0.9
            },
            'insurance': {
                'keywords': ['assurance', 'assureur', 'sinistre', 'indemnisation', 'prime', 'franchise'],
                'patterns': [r'police\s+d\'assurance', r'contrat\s+d\'assurance'],
                'weight': 1.1
            },
            'penalties': {
                'keywords': ['pÃ©nalitÃ©', 'amende', 'sanction', 'dommages-intÃ©rÃªts', 'astreinte', 'indemnitÃ©'],
                'patterns': [r'pÃ©nalitÃ©s?\s+de', r'\d+[\.,]?\d*\s*%\s*par'],
                'weight': 1.2
            }
        }

        scores = {}
        for category, data in weighted_categories.items():
            # Score des mots-clÃ©s
            keyword_score = sum(1 for keyword in data['keywords'] if keyword in content_lower)

            # Score des patterns
            pattern_score = sum(len(re.findall(pattern, content_lower)) for pattern in data.get('patterns', []))

            # Score pondÃ©rÃ© final
            total_score = (keyword_score + pattern_score * 1.5) * data['weight']

            if total_score > 0:
                scores[category] = total_score

        if scores:
            # Seuil minimum pour Ã©viter les fausses classifications
            max_score = max(scores.values())
            if max_score >= 1.5:  # Seuil de confiance
                return max(scores, key=scores.get)

        return 'general'

    def get_classification_scores(self, content: str) -> Dict:
        """Obtenir les scores de classification pour tous les types."""
        content_lower = content.lower()

        categories = {
            'obligations': ['obligation', 'engage', 'doit', 'responsabilitÃ©'],
            'conditions': ['condition', 'clause', 'modalitÃ©', 'si'],
            'financial': ['prix', 'coÃ»t', 'euros', 'â‚¬', 'montant'],
            'timeline': ['dÃ©lai', 'date', 'Ã©chÃ©ance', 'livraison'],
            'guarantees': ['garantie', 'assurance', 'caution'],
            'technical_requirements': ['technique', 'norme', 'spÃ©cification'],
            'quality_control': ['contrÃ´le', 'vÃ©rification', 'test'],
            'safety_security': ['sÃ©curitÃ©', 'protection', 'risque'],
            'administrative': ['autorisation', 'permis', 'dÃ©claration'],
            'definitions': ['dÃ©finition', 'signifie', 'dÃ©signe'],
            'procedures': ['procÃ©dure', 'mÃ©thode', 'Ã©tape']
        }

        scores = {}
        for category, keywords in categories.items():
            scores[category] = sum(1 for keyword in keywords if keyword in content_lower)

        return scores

    def calculate_coherence(self, content: str) -> float:
        """Calculer la cohÃ©rence du contenu."""
        sentences = re.split(r'[.!?]+', content)
        if len(sentences) < 2:
            return 0.8

        # VÃ©rifier la prÃ©sence de connecteurs
        connectors = ['et', 'ou', 'mais', 'donc', 'car', 'ainsi', 'alors', 'cependant', 'toutefois']
        connector_count = sum(1 for conn in connectors if conn in content.lower())

        coherence = min(1.0, 0.6 + (connector_count / len(sentences)) * 0.4)
        return round(coherence, 3)

    def calculate_relevance(self, content: str) -> float:
        """Calculer la pertinence juridique."""
        legal_terms = [
            'contrat', 'accord', 'convention', 'engagement', 'obligation',
            'droit', 'devoir', 'clause', 'article', 'disposition'
        ]

        words = content.lower().split()
        legal_word_count = sum(1 for word in words if word in legal_terms)

        relevance = min(1.0, 0.7 + (legal_word_count / len(words)) * 3)
        return round(relevance, 3)

    def calculate_factual_density(self, entities: Dict) -> float:
        """Calculer la densitÃ© factuelle basÃ©e sur les entitÃ©s."""
        total_entities = sum(len(entity_list) for entity_list in entities.values())
        # Normaliser par rapport Ã  un chunk moyen
        density = min(1.0, total_entities / 5)
        return round(density, 3)

    def get_title(self, content: str) -> str:
        """Extraire ou gÃ©nÃ©rer un titre pour le chunk."""
        # Chercher un article numÃ©rotÃ©
        article_match = re.search(r'article\s+\d+[^\n.]*', content, re.IGNORECASE)
        if article_match:
            return article_match.group(0).strip()

        # Chercher une clause
        clause_match = re.search(r'clause\s+[^\n.]*', content, re.IGNORECASE)
        if clause_match:
            return clause_match.group(0).strip()

        # PremiÃ¨re phrase courte
        sentences = re.split(r'[.!?]+', content)
        if sentences and len(sentences[0]) < 80:
            return sentences[0].strip()

        return "Clause contractuelle"

    def extract_key_elements(self, content: str) -> list:
        """Extraire les Ã©lÃ©ments clÃ©s du chunk."""
        elements = []

        # Mots-clÃ©s importants avec prioritÃ©
        priority_words = {
            'contrat': 3, 'prix': 3, 'dÃ©lai': 3, 'garantie': 3,
            'obligation': 2, 'clause': 2, 'article': 2, 'conditions': 2,
            'paiement': 2, 'livraison': 2, 'responsabilitÃ©': 2,
            'assurance': 1, 'modalitÃ©': 1, 'Ã©chÃ©ance': 1
        }

        content_lower = content.lower()
        word_scores = []

        for word, priority in priority_words.items():
            if word in content_lower:
                word_scores.append((word, priority))

        # Trier par prioritÃ© et prendre les 5 premiers
        word_scores.sort(key=lambda x: x[1], reverse=True)
        elements = [word for word, _ in word_scores[:5]]

        return elements

    def detect_document_type(self, text: str) -> str:
        """DÃ©tection du type de document."""
        text_lower = text.lower()

        patterns = {
            'contrat_reservation_vefa': ['vefa', 'rÃ©servation', 'futur achÃ¨vement'],
            'cctp': ['cctp', 'clauses techniques particuliÃ¨res', 'cahier des charges'],
            'acte_notarie': ['acte', 'notaire', 'notariÃ©', 'Ã©tude'],
            'bail_habitation': ['bail', 'location', 'logement', 'loyer'],
            'bail_commercial': ['bail commercial', 'fonds de commerce'],
            'marche_public': ['marchÃ© public', 'appel d\'offres', 'soumission'],
            'permis_construire': ['permis de construire', 'autorisation', 'urbanisme'],
            'devis': ['devis', 'estimation', 'chiffrage'],
            'facture': ['facture', 'facturation', 'tva']
        }

        for doc_type, keywords in patterns.items():
            if any(keyword in text_lower for keyword in keywords):
                return doc_type

        return 'contrat_general'

# Instance du service avec initialisation
chunking_service = ChunkingService()

@app.get("/")
async def root():
    """Point d'entrÃ©e principal de l'API."""
    return {
        "message": "Legal Document Chunking API",
        "version": "1.0.0",
        "endpoints": {
            "chunk": "/chunk - POST - Chunking de documents",
            "health": "/health - GET - Status de l'API"
        }
    }

@app.get("/health")
async def health():
    """Endpoint de santÃ© pour vÃ©rification."""
    return {
        "status": "healthy",
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

@app.post("/chunk")
async def chunk_document(request: ChunkingRequest):
    """Endpoint principal pour le chunking de documents."""
    try:
        extracted_text = request.extractedText.strip()

        if not extracted_text:
            raise HTTPException(
                status_code=400,
                detail="Le champ extractedText est requis et ne peut pas Ãªtre vide"
            )

        if len(extracted_text) < 100:
            raise HTTPException(
                status_code=400,
                detail="Le texte doit contenir au moins 100 caractÃ¨res"
            )

        # Options avec valeurs par dÃ©faut
        options = request.options
        target_chunk_size = options.get('target_chunk_size', 60)
        overlap_size = options.get('overlap_size', 15)

        # Validation des paramÃ¨tres
        if not (20 <= target_chunk_size <= 200):
            raise HTTPException(
                status_code=400,
                detail="target_chunk_size doit Ãªtre entre 20 et 200 mots"
            )

        if not (0 <= overlap_size <= 50):
            raise HTTPException(
                status_code=400,
                detail="overlap_size doit Ãªtre entre 0 et 50 mots"
            )

        # Traitement du chunking
        start_time = time.time()

        chunks = chunking_service.create_smart_chunks(
            extracted_text,
            target_chunk_size,
            overlap_size
        )

        end_time = time.time()
        processing_time = int((end_time - start_time) * 1000)

        # Calcul des statistiques
        quality_dist = {'high': 0, 'medium': 0, 'low': 0}
        total_quality = 0

        for chunk in chunks:
            score = chunk['quality_analysis']['overall_score']
            total_quality += score

            if score >= 0.8:
                quality_dist['high'] += 1
            elif score >= 0.5:
                quality_dist['medium'] += 1
            else:
                quality_dist['low'] += 1

        avg_quality = total_quality / len(chunks) if chunks else 0

        # Validation des rÃ©sultats
        low_quality_rate = (quality_dist['low'] / len(chunks) * 100) if chunks else 0

        validation_results = {
            'consistency_score': 0.85,
            'cross_reference_integrity': 0.90,
            'missing_elements': [],
            'recommendations': []
        }

        if low_quality_rate > 30:
            validation_results['recommendations'].append(
                "Taux Ã©levÃ© de chunks de basse qualitÃ© - considÃ©rer l'augmentation de target_chunk_size"
            )

        # RÃ©ponse finale
        response_data = {
            'success': True,
            'chunks': chunks,
            'document_stats': {
                'document_type': chunking_service.detect_document_type(extracted_text),
                'estimated_complexity': 'medium',
                'processing_time_ms': processing_time,
                'total_chunks': len(chunks),
                'avg_chunk_quality': round(avg_quality, 3),
                'quality_distribution': quality_dist,
                'text_length': len(extracted_text),
                'avg_chunk_size': sum(chunk['metadata']['word_count'] for chunk in chunks) / len(chunks) if chunks else 0
            },
            'validation_results': validation_results,
            'processing_info': {
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'version': '1.0.0',
                'target_chunk_size': target_chunk_size,
                'overlap_size': overlap_size
            }
        }

        return JSONResponse(content=response_data)

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Erreur interne du serveur: {str(e)}"
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)